---
layout:     post
title:     Transactions for the REST of Us
subtitle:   Transactions for the REST of Us
date:       2018-09-22
author:     老张
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - TCC
    - Transactions 
    - REST 
typora-copy-images-to: ..\img
typora-root-url: ..
---

# 为什么说B+树比B树更适合做文件索引

## 概述

这里续一下上次讲索引时提的问题：为什么说B+树比B树更适合做文件索引呢？下面先从两张图来介绍下B树和B+树，然后再说下原因。

------

## B树

![mysql_20190308090358](/img/mysql_20190308090358.jpg)



## B+树



![mysql_20190308090421](/img/mysql_20190308090421.jpg)

从上面两张图我们可以发现以下区别：

**1、结构上**

- B树中关键字集合分布在整棵树中，叶节点中不包含任何关键字信息，而B+树关键字集合分布在叶子结点中，非叶节点只是叶子结点中关键字的索引；
- B树中任何一个关键字只出现在一个结点中，而B+树中的关键字必须出现在叶节点中，也可能在非叶结点中重复出现；

**2、性能上**

- 不同于B树只适合**随机检索**，B+树同时支持随机检索**和顺序检索**；
- B+树的磁盘读写代价更低。B+树的**内部结点并没有指向关键字具体信息的指针**，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素。
- B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。
- （数据库索引采用B+树的主要原因是，）B-树在提高了磁盘IO性能的同时并没有解决**元素遍历的效率**低下的问题。B+树的叶子节点使用指针顺序连接在一起，**只要遍历叶子节点就可以实现整棵树的遍历**。而且**在数据库中基于范围的查询是非常频繁的**，而B树不支持这样的操作（或者说效率太低）。

------

## B+树比B树更适合做文件索引**原因**

**1、B+树空间利用率更高，可减少I/O次数**，

一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。而因为B+树的内部节点只是作为索引使用，而不像B-树那样每个节点都需要存储硬盘指针。

也就是说：B+树中每个非叶节点没有指向某个关键字具体信息的指针，所以每一个节点可以存放更多的关键字数量，即一次性读入内存所需要查找的关键字也就越多，减少了I/O操作。

e.g.假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree(一个结点最多8个关键字)的内 部结点需要2个盘快。而B+ 树内部结点只需要1个盘快。当需要把内部结点读入内存中的时候，B 树就比B+ 树多一次盘块查找时间(在磁盘中就 是 盘片旋转的时间)。

**2、增删文件（节点）时，效率更高，**

因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。

**3、B+树的查询效率更加稳定，**

因为B+树的每次查询过程中，都需要遍历从根节点到叶子节点的某条路径。所有关键字的查询路径长度相同，导致每一次查询的效率相当。

------

## **MySQL的B-Tree索引（技术上说B+Tree）**

在**MySQL** 中，主要有四种类型的索引，分别为： B-Tree 索引， Hash 索引， Fulltext 索引和 R-Tree 索引。我们主要分析B-Tree 索引。



B-Tree 索引是 MySQL 数据库中使用最为频繁的索引类型，除了 Archive 存储引擎之外的其他所有的存储引擎都支持 B-Tree 索引。Archive 引擎直到 MySQL 5.1 才支持索引，而且只支持索引单个 AUTO_INCREMENT 列。

![mysql_20190308090433](/img/mysql_20190308090433.jpg)

不仅仅在 MySQL 中是如此，实际上在其他的很多数据库管理系统中B-Tree 索引也同样是作为最主要的索引类型，这主要是因为 B-Tree 索引的存储结构在数据库的数据检索中有非常优异的表现。

一般来说， MySQL 中的 B-Tree 索引的物理文件大多都是以 Balance Tree 的结构来存储的，也就是所有实际需要的数据都存放于 Tree 的 Leaf Node(叶子节点) ，而且到任何一个 Leaf Node 的最短路径的长度都是完全相同的，所以我们大家都称之为 B-Tree 索引。当然，可能各种数据库（或 MySQL 的各种存储引擎）在存放自己的 B-Tree 索引的时候会对存储结构稍作改造。如 Innodb 存储引擎的 B-Tree 索引实际使用的存储结构实际上是 B+Tree，也就是在 B-Tree 数据结构的基础上做了很小的改造，在每一个Leaf Node 上面出了存放索引键的相关信息之外，还存储了指向与该 Leaf Node 相邻的后一个 LeafNode 的指针信息（增加了顺序访问指针），这主要是为了加快检索多个相邻 Leaf Node 的效率考虑。

------



![linux_perf_tools_full](/img/linux_perf_tools_full.png)





# 全面的InnoDB锁机制

抱歉，没早点把这么全面的InnoDB锁机制发给你

**作者介绍**

**崔亚盟，**阿里高级开发工程师，目前在研究InnoDB相关技术，乐于专研学道，是技术分享的践行者。

数据事务设计遵循ACID的原则： 原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。一个支持事务（Transaction）的数据库，必须要具有这四种特性，否则在事务过程（Transaction processing）当中无法保证数据的正确性。

MySQL数据库提供了四种默认的隔离级别，读未提交(read-uncommitted)、读已提交(或不可重复读)(read-committed)、可重复读(repeatable-read)、串行化(serializable)。

MySQL的默认隔离级别是RR。

**一、锁基本概念**

1、共享锁和排它锁

InnoDB实现了两种标准行级锁，一种是共享锁(shared locks，S锁)，另一种是独占锁，或者叫排它锁(exclusive locks，X锁)。

S锁允许当前持有该锁的事务读取行。X锁允许当前持有该锁的事务更新或删除行。

**S锁**：如果事务T1持有了行r上的S锁，则其他事务可以同时持有行r的S锁，但是不能对行r加X锁。

**X锁：**如果事务T1持有了行r上的X锁，则其他任何事务不能持有行r的X锁，必须等待T1在行r上的X锁释放。

如果事务T1在行r上保持S锁，则另一个事务T2对行r的锁的请求按如下方式处理：

- T2可以同时持有S锁；
- T2如果想在行r上获取X锁，必须等待其他事务对该行添加的S锁或X锁的释放。

2、意向锁-Intention Locks

InnoDB支持多种粒度的锁，允许行级锁和表级锁的共存。例如LOCK TABLES ... WRITE等语句可以在指定的表上加上独占锁。InnoBD使用意向锁来实现多个粒度级别的锁定。意向锁是表级锁，表示table中的row所需要的锁(S锁或X锁)的类型。

意向锁分为意向共享锁(IS锁)和意向排它锁(IX锁)。IS锁表示当前事务意图在表中的行上设置共享锁，下面语句执行时会首先获取IS锁，因为这个操作在获取S锁：

SELECT ... LOCK IN SHARE MODE

IX锁表示当前事务意图在表中的行上设置排它锁。下面语句执行时会首先获取IX锁，因为这个操作在获取X锁：

SELECT ... FOR UPDATE

事务要获取某个表上的S锁和X锁之前，必须先分别获取对应的IS锁和IX锁。

3、锁的兼容性

锁的兼容矩阵如下：

![innodb-lock-_20190318084316](/img/innodb-lock-_20190318084316.jpg)

按照上面的兼容性，如果不同事务之间的锁兼容，则当前加锁事务可以持有锁，如果有冲突则会等待其他事务的锁释放。

如果一个事务请求锁时，请求的锁与已经持有的锁冲突而无法获取时，互相等待就可能会产生死锁。

意向锁不会阻止除了全表锁定请求之外的任何锁请求。 意向锁的主要目的是显示事务正在锁定某行或者正意图锁定某行。

**二、InnoDB中的锁**

常见的锁有Record锁、gap锁、next-key锁、插入意向锁、自增锁等。下面会对每一种锁给出一个查看锁的示例：

1、准备工作



#### **测试用表结构**

示例的基础是一个只有两列的数据库表：

![innodb-lock-_20190318084402](/img/innodb-lock-_20190318084402.jpg)

数据表test只有两列，id是主键索引，code是普通的索引(注意，一定不要是唯一索引)，并初始化了两条记录，分别是(1,1),(10,10)。这样，我们验证唯一键索引就可以使用id列，验证普通索引(非唯一键二级索引)时就使用code列。

#### **查看锁状态的方式**

要看到锁的情况，必须手动开启多个事务，其中一些锁的状态的查看则必须使锁处于waiting状态，这样才能在mysql的引擎状态日志中看到。

命令：

mysql> show engine innodb status;

这条命令能显示最近几个事务的状态、查询和写入情况等信息。当出现死锁时，命令能给出最近的死锁明细。

2、记录锁Record Locks



#### **Record锁**

Record Lock是对索引记录的锁定。

记录锁有两种模式：S模式和X模式。例如：SELECT id FROM test WHERE id = 10 FOR UPDATE; 表示防止任何其他事务插入、更新或者删除id =10的行。

记录锁始终只锁定索引。即使表没有建立索引，InnoDB也会创建一个隐藏的聚簇索引(隐藏的递增主键索引)，并使用此索引进行记录锁定。



#### **查看记录锁**

开启第一个事务，不提交，测试完之后回滚。

![innodb-lock-_20190318090202](/img/innodb-lock-_20190318090202.jpg)

事务加锁情况：
![innodb-lock-_20190318090245](/img/innodb-lock-_20190318090245.jpg)

可以看到有一行被加了锁。由之前对锁的描述可以推测出，update语句给id=1这一行上加了一个X锁。

注意：X锁广义上是一种抽象意义的排它锁，即锁一般分为X模式和S模式，狭义上指row或者index上的锁，而Record锁是索引上的锁。 为了不修改数据，可以用select ... for update语句，加锁行为和update、delete是一样的，insert加锁机制较为复杂，后面的章节会提到。

第一个事务保持原状，不要提交或者回滚，现在开启第二个事务：

![innodb-lock-片_20190318090550](/img/innodb-lock-片_20190318090550.jpg)

执行update时，sql语句的执行被阻塞了。查看下事务状态:

![innodb-lock-_20190318090650](/img/innodb-lock-_20190318090650.jpg)



喜闻乐见，我们看到了这个锁的状态。状态标题是'事务正在等待获取锁'，描述中的lock_mode X locks rec but not gap就是本章节中的record记录锁，直译一下'X锁模式锁住了记录'。后面还有一句but not gap意思是只对record本身加锁，并不对间隙加锁，间隙锁的叙述见下面的内容。

3、间隙锁Gap Locks



#### **间隙锁**

间隙锁作用在索引记录之间的间隔，又或者作用在第一个索引之前，最后一个索引之后的间隙。不包括索引本身。

例如：

SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE;

这条语句阻止其他事务插入10和20之间的数字，无论这个数字是否存在。

间隙可以跨越0个，单个或多个索引值。

间隙锁是性能和并发权衡的产物，只存在于部分事务隔离级别。

select * from table where id=1;

唯一索引可以锁定一行，所以不需要间隙锁锁定。如果列没有索引或者具有非唯一索引，该语句会锁定当前索引前的间隙。

在同一个间隙上，不同的事务可以持有上述兼容/冲突表中冲突的两个锁。例如，事务T1现在持有一个间隙S锁，T2可以同时在同一个间隙上持有间隙X锁。

允许冲突的锁在间隙上锁定的原因是，如果从索引中清除一条记录，则由不同事务在这条索引记录上的加间隙锁的动作必须被合并。

InnoDB中的间隙锁的唯一目的是防止其他事务插入间隙。间隙锁是可以共存的，一个事务占用的间隙锁不会阻止另一个事务获取同一个间隙上的间隙锁。

如果事务隔离级别改为RC，则间隙锁会被禁用。



#### **查看间隙锁**

按照官方文档，where子句查询条件是唯一键且指定了值时，只有record锁，没有gap锁。

如果where语句指定了范围，gap锁是存在的。

这里只测试验证一下当指定非唯一键索引的时候，gap锁的位置，按照文档的说法，会锁定当前索引及索引之前的间隙。(指定了非唯一键索引，例如code=10，间隙锁仍然存在)

开启第一个事务，锁定一条非唯一的普通索引记录：

![innodb-lock-_20190318090741](/img/innodb-lock-_20190318090741.jpg)

由于预存了两条数据，row(1,1)和row(10,10)，此时这个间隙应该是1<gap<10。我们先插入row(2,2)来验证下gap锁的存在，再插入row(0,0)来验证gap的边界。

按照间隙锁的官方文档定义，select * from test where code = 10 for update; 会锁定code=10这个索引，并且会锁定code<10的间隙。

开启第二个事务，在code=10之前的间隙中插入一条数据，看下这条数据是否能够插入：

![innodb-lock-_20190318090816](/img/innodb-lock-_20190318090816.jpg)

插入的时候，执行被阻塞，查看引擎状态：

![innodb-lock-_20190318090943](/img/innodb-lock-_20190318090943.jpg)



插入语句被阻塞了，lock_mode X locks gap before rec，由于第一个事务锁住了1到10之间的gap，需要等待获取锁之后才能插入。

如果再开启一个事务，插入(0,0)：



![innodb-lock_20190318091026](/img/innodb-lock_20190318091026.jpg)

可以看到：指定的非唯一建索引的gap锁的边界是当前索引到上一个索引之间的gap。

最后给出锁定区间的示例，首先插入一条记录(5,5)：

![innodb_20190318091110](/img/innodb_20190318091110.jpg)

开启第一个事务：



![innodb_20190318091208](/img/innodb_20190318091208.jpg)

第二个事务，试图去更新code=5的行：

![innodb_20190318092259](/img/innodb_20190318092259.jpg)

执行到这里，如果第一个事务不提交或者回滚的话，第二个事务一直等待直至mysql中设定的超时时间。

4、Next-key Locks



#### **Next-key锁**

Next-key锁实际上是Record锁和gap锁的组合。Next-key锁是在下一个索引记录本身和索引之前的gap加上S锁或是X锁(如果是读就加上S锁，如果是写就加X锁)。

默认情况下，InnoDB的事务隔离级别为RR，系统参数innodb_locks_unsafe_for_binlog的值为false。InnoDB使用next-key锁对索引进行扫描和搜索，这样就读取不到幻象行，避免了幻读的发生。

幻读是指在同一事务下，连续执行两次同样的SQL语句，第二次的SQL语句可能会返回之前不存在的行。

当查询的索引是唯一索引时，Next-key lock会进行优化，降级为Record Lock，此时Next-key lock仅仅作用在索引本身，而不会作用于gap和下一个索引上。



#### **查看Next-key锁**

- ##### Next-key锁的作用范围

如上述例子，数据表test初始化了row(1,1),row(10,10)，然后插入了row(5,5)。数据表如下：

![innodb_20190318092337](/img/innodb_20190318092337.jpg)

由于id是主键、唯一索引，mysql会做优化，因此使用code这个非唯一键的二级索引来举例说明。对于code，可能的next-key锁的范围是：

(-∞,1]

(1,5]

(5,10]

(10,+∞)

开启第一个事务，在code=5的索引上请求更新：

![innodb_20190318092406](/img/innodb_20190318092406.jpg)

之前在gap锁的章节中介绍了，code=5 for update会在code=5的索引上加一个record锁，还会在1<gap<5的间隙上加gap锁。现在不再验证，直接插入一条(8,8)：

![innodb_20190318092440](/img/innodb_20190318092440.jpg)

insert处于等待执行的状态，这就是next-key锁生效而导致的结果。第一个事务，锁定了区间(1,5]，由于RR的隔离级别下next-key锁处于开启生效状态，又锁定了(5,10]区间。所以插入SQL语句的执行被阻塞。

解释：在这种情况下，被锁定的区域是code=5前一个索引到它的间隙，以及next-key的区域。code=5 for update对索引的锁定用区间表示，gap锁锁定了(1,5)，record锁锁定了{5}索引记录，next-key锁锁住了(5,10]，也就是说整个(1,10]的区间被锁定了。由于是for update，所以这里的锁都是X锁，因此阻止了其他事务中带有冲突锁定的操作执行。

如果我们在第一个事务中，执行了code>8 for update，在扫描过程中，找到了code=10，此时就会锁住10之前的间隙(5到10之间的gap)，10本身(record)，和10之后的间隙(next-key)。此时另一个事务插入(6,6),(9,9)和(11,11)都是不被允许的，只有在前一个索引5及5之前的索引和间隙才能执行插入(更新和删除也会被阻塞)。

5、插入意向锁

插入意向锁在行插入之前由INSERT设置一种间隙锁，是意向排它锁的一种。在多事务同时写入不同数据至同一索引间隙的时，不会发生锁等待，事务之间互相不影响其他事务的完成，这和间隙锁的定义是一致的。

假设一个记录索引包含4和7，其他不同的事务分别插入5和6，此时只要行不冲突，插入意向锁不会互相等待，可以直接获取。参照锁兼容/冲突矩阵。

插入意向锁的例子不再列举，可以查看gap锁的第一个例子。

6、自增锁

自增锁(AUTO-INC Locks)是事务插入时自增列上特殊的表级别的锁。最简单的一种情况：如果一个事务正在向表中插入值，则任何其他事务必须等待，以便第一个事务插入的行接收连续的主键值。

我们一般把主键设置为AUTO_INCREMENT的列，默认情况下这个字段的值为0，InnoDB会在AUTO_INCREMENT修饰下的数据列所关联的索引末尾设置独占锁。

在访问自增计数器时，InnoDB使用自增锁，但是锁定仅仅持续到当前SQL语句的末尾，而不是整个事务的结束，毕竟自增锁是表级别的锁，如果长期锁定会大大降低数据库的性能。由于是表锁，在使用期间，其他会话无法插入表中。

**三、幻读**

这一部分，我们将通过幻读，逐步展开对InnoDB锁的探究。

1、幻读概念

解释了不同概念的锁的作用域，我们来看一下幻读到底是什么。

幻读在RR条件下是不会出现的。因为RR是Repeatable Read，它是一种事务的隔离级别，直译过来也就是“在同一个事务中，同样的查询语句的读取是可重复”，也就是说他不会读到”幻影行”(其他事务已经提交的变更)，它读到的只能是重复的(无论在第一次查询之后其他事务做了什么操作，第二次查询结果与第一次相同)。

上面的例子都是使用for update，这种读取操作叫做当前读，对于普通的select语句均为快照读。

当前读，又叫加锁读，或者阻塞读。这种读取操作不再是读取快照，而是读取最新版本并且加锁。快照读不会添加任何锁。

官方文档对于幻读的定义是这样的：

原文：The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom”row.

手动无脑翻译：所谓的幻影行问题是指，在同一个事务中，同样的查询语句执行多次，得到了不同的结果，这就是幻读。例如：如果同一个SELECT语句执行了两次，第二次执行的时候比第一次执行时多出一行，则该行就是所谓的幻影行。

其中这一句：

“The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times.”

这看起来应该是不可重复读的定义，同样的查询得到了不同的结果(两次结果不是重复的)，但是后面的举例给出了幻读真正的定义，第二次比第一次多出了一行。

也就是说，幻读的出现有这样一个前提，第二次查询前其他事务提交了一个INSERT插入语句。而不可重复读出现的前提是第二次查询前其他事务提交了UPDATE或者DELETE操作。

mysql的快照读，使得在RR的隔离级别上在next-Key的作用区间内，制造了一个快照副本，这个副本是隔离的，无论副本对应的区间里的数据被其他事务如何修改，在当前事务中，取到的数据永远是副本中的数据。

RR级别下之所以可以读到之前版本的数据，是由于数据库的MVCC(Multi-Version Concurrency Control，多版本并发控制)。

参见InnoDB Multi-Versioning

https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html

有些文章中提到“RR也不能完全避免幻读”，实际上官方文档实际要表达的意义是“在同一个事务内，多次连续查询的结果是一样的，不会因其他事务的修改而导致不同的查询结果”，这里先给出实验结论：

- 当前事务如果未发生更新操作(增删改)，快照版本会保持不变，多次查询读取的副本是同一个；
- 当前事务如果发生更新(增删改)，再次查询时，会刷新快照版本。

2、RC级别下的幻读

RC情况下会出现幻读。首先设置隔离级别为RC：

SET SESSION tx_isolation='READ-COMMITTED';

![innodb_20190318092516](/img/innodb_20190318092516.jpg)

RC(Read Commit)隔离级别可以避免脏读，事务内无法获取其他事务未提交的变更，但是由于能够读到已经提交的事务，因此会出现幻读和不重复读。也就是说，RC的快照读是读取最新版本数据，而RR的快照读是读取被next-key锁作用区域的副本。

3、RR级别下能否避免幻读？

我们先来模拟一下RR隔离级别下没有出现幻读的情况：

开启第一个事务并执行一次快照查询：

![innodb_20190318092546](/img/innodb_20190318092546.jpg)

这两个事务的执行，有两个问题：

- 为什么之前的例子中，在第二个事务的INSERT被阻塞了，而这次却执行成功了。

  这是因为原来的语句中带有for update，这种读取是当前读，会加锁。而本次第一个事务中的SELECT仅仅是快照读，没有加任何锁。所以不会阻塞其他的插入。

- 数据库中的数据已经改变，为什么会读不到？

  这个就是之前提到的next-key lock锁定的副本。RC及以下级别才会读到已经提交的事务。更多的业务逻辑是希望在某段时间内或者某个特定的逻辑区间中，前后查询到的数据是一致的，当前事务是和其他事务隔离的。这也是数据库在设计实现时遵循的ACID原则。

再给出RR条件下出现幻读的情形，这种情形不需要两个事务，一个事务就已经可以说明：

![innodb_20190318092617](/img/innodb_20190318092617.jpg)

至于RR隔离级别下到底会不会出现幻读，就需要看幻读的定义中的查询到底是连续的查询还是不连续的查询。如果认为RR级别下可能会出现幻读，那该级别下也会出现不重复读。

RR隔离级别下，虽然不会出现幻读，但是会因此产生其他的问题。

前提：当前数据表中只存在(1,1),(5,5),(10,10)三组数据。

如果数据库隔离级别不是默认，可以执行SET SESSION tx_isolation='REPEATABLE-READ';(该语句不是全局设置)更新为RR。

然后执行下列操作：

![innodb_20190318092649](/img/innodb_20190318092649.jpg)

4、更新丢失(Lost Update)



#### **更新丢失**

除了上述这类问题外，RR还会有丢失更新的问题。如下表给出的操作：

![innodb_20190318092747](/img/innodb_20190318092747.jpg)

这个例子里，事务一的更新是无效的，尽管在这个事务里程序认为还存在(10,10)记录。事务一中更新之前的SELECT操作是快照读，所以读到了快照里的(10,10)，而UPDATE中的WHERE子句是当前读，取得是最新版本的数据，所以matched: 0 Changed: 0

如果上述例子中的操作是对同一条记录做修改，就会引起更新丢失。例如，事务一和二同时开启，事务一先执行update test set code=100 where id=10;，事务二再执行update test set code=200 where id=10;，事务一的更新就会被覆盖。

这就是经典的丢失更新问题，英文叫Lost Update，又叫提交覆盖，因为是最后执行更新的事务提交导致的覆盖。还有一种更新丢失叫做回滚覆盖，即一个事务的回滚把另一个事务提交的数据给回滚覆盖了，但是目前市面上所有的数据库都不支持这种stupid的操作，因此不再详述。



#### **乐观锁与悲观锁**

这种情况下，引入我们常见的两种方式来解决该问题：

- 乐观锁：在UPDATE的WHERE子句中加入版本号信息来确定修改是否生效；
- 悲观锁：在UPDATE执行前，SELECT后面加上FOR UPDATE来给记录加锁，保证记录在UPDATE前不被修改。SELECT ... FOR UPDATE是加上了X锁，也可以通过SELECT ... LOCK IN SHARE MODE加上S锁，来防止其他事务对该行的修改。

无论是乐观锁还是悲观锁，使用的思想都是一致的，那就是当前读。乐观锁利用当前读判断是否是最新版本，悲观锁利用当前读锁定行。

但是使用乐观锁时仍然需要非常谨慎，因为RR是可重复读的，一定不能在UPDATE之前先把版本号使用快照读获取出来。

**四、InnoDB对不同语句执行时**

**的加锁状况**

如果一个SQL语句要对二级索引(非主键索引)设置X模式的Record锁，InnoDB还会检索出相应的聚簇索引(主键索引)并对它们设置锁定。

1、SELECT ... FROM...不加锁

SELECT ... FROM是快照读取，除了SERIALIZABLE的事务隔离级别，该SQL语句执行时不会加任何锁。

SERIALIZABLE级别下，SELECT语句的执行会在遇到的索引记录上设置S模式的next-key锁。但是对于唯一索引，只锁定索引记录，而不会锁定gap。

2、UPDATE系列

S锁读取(SELECT ... LOCK IN SHARE MODE)，X锁读取(SELECT ... FOR UPDATE)、更新UPDATE和删除DELETE这四类语句，采用的锁取决于搜索条件中使用的索引类型。

- 如果使用唯一索引，InnoDB仅锁定索引记录本身，不锁定间隙；
- 如果使用非唯一索引，或者未命中索引，InnoDB使用间隙锁或者next-key锁来锁定索引范围，这样就可以阻止其他事务插入锁定范围。



#### **UPDATE语句**

UPDATE ... WHERE ... 在搜索遇到的每条记录上设置一个独占的next-key锁，如果是唯一索引只锁定记录。

当UPDATE修改聚簇索引时，将对受影响的二级索引采用隐式锁，隐式锁是在索引中对二级索引的记录逻辑加锁，实际上不产生锁对象，不占用内存空间。

例如update test set code=100 where id=10;执行的时候code=10的索引(code是二级索引，见文中给出的建表语句)会被加隐式锁，只有隐式锁产生冲突时才会变成显式锁(如S锁、X锁)。即此时另一个事务也去更新id=10这条记录，隐式锁就会升级为显示锁。

这样做的好处是降低了锁的开销。

UPDATE可能会导致新的普通索引的插入。当新的索引插入之前，会首先执行一次重复索引检查。在重复检查和插入时，更新操作会对受影响的二级索引记录采用共享锁定(S锁)。



#### **DELETE语句**

DELETE FROM ... WHERE ... 在搜索遇到的每条记录上设置一个独占的next-key锁，如果是唯一索引只锁定记录。

3、INSERT

INSERT区别于UPDATE系列单独列出，是因为它的处理方式较为特别。

插入行之前，会设置一种插入意向锁，插入意向锁表示插入的意图。如果其它事务在要插入的位置上设置了X锁，则无法获取插入意向锁，插入操作也因此阻塞。

INSERT在插入的行上设置X锁。该锁是一个Record锁，并不是next-key锁，即只锁定记录本身，不锁定间隙，因此不会阻止其他会话在这行记录前的间隙中插入新的记录。具体的加锁过程见下文。

五、**可能的死锁场景**

1、Duplicate key error引发的死锁

并发条件下，唯一键索引冲突可能会导致死锁，这种死锁一般分为两种：一种是rollback引发，另一种是commit引发。



#### **rollback引发的Duplicate key死锁**

我命名为insert-insert-insert-rollback死锁：

![innodb_20190318092827](/img/innodb_20190318092827.jpg)

当事务一执行回滚时，事务二和事务三发生了死锁。InnoDB的死锁检测一旦检测到死锁发生，会自动失败其中一个事务，因此看到的结果是一个失败另一个成功。

- 为什么会死锁？

死锁产生的原因是事务一插入记录时，对(2,2)记录加X锁，此时事务二和事务三插入数据时检测到了重复键错误，事务二和事务三要在这条索引记录上设置S锁，由于X锁的存在，S锁的获取被阻塞。

事务一回滚，由于S锁和S锁是可以兼容的，因此事务二和事务三都获得了这条记录的S锁。此时其中一个事务希望插入，则该事务期望在这条记录上加上X锁，然而另一个事务持有S锁，S锁和X锁互相是不兼容的，两个事务就开始互相等待对方的锁释放，造成了死锁。

- 事务二和事务三为什么会加S锁，而不是直接等待X锁？

事务一的insert语句加的是隐式锁(隐式的Record锁、X锁)，但是其他事务插入同一行记录时，出现了唯一键冲突，事务一的隐式锁升级为显示锁。

事务二和事务三在插入之前判断到了唯一键冲突，是因为插入前的重复索引检查，这次检查必须进行一次当前读，于是非唯一索引就会被加上S模式的next-key锁，唯一索引就被加上了S模式的Record锁。

因为插入和更新之前都要进行重复索引检查而执行当前读操作，所以RR隔离级别下，同一个事务内不连续的查询，可能也会出现幻读的效果(但个人并不认为RR级别下也会出现幻读，幻读的定义应该是连续的读取)。而连续的查询由于都是读取快照，中间没有当前读的操作，所以不会出现幻读。



#### **commit引发的Duplicate key死锁**

delete-insert-insert-commit死锁：

![innodb_20190318092913](/img/innodb_20190318092913.jpg)

这种情况下产生的死锁和insert-insert-insert-rollback死锁产生的原理一致。

2、数据插入的过程

经过以上分析，一条数据在插入时经过以下几个过程：

假设数据表test.test中存在(1,1)、(5,5)和(10,10)三条记录。

- 事务开启，尝试获取插入意向锁。例如，事务一执行了select * from test where id>8 for update，事务二要插入(9,9)，此时先要获取插入意向锁，由于事务一已经在对应的记录和间隙上加了X锁，因此事务二被阻塞，并且阻塞的原因是获取插入意向锁时被事务一的X锁阻塞。
- 获取意向锁之后，插入之前进行重复索引检查。重复索引检查为当前读，需要添加S锁。
- 如果是已经存在唯一索引，且索引未加锁。直接抛出Duplicate key的错误。如果存在唯一索引，且索引加锁，等待锁释放。
- 重复检查通过之后，加入X锁，插入记录

3、GAP与Insert Intention冲突引发死锁



#### **update-insert死锁**

仍然是表test，当前表中的记录如下:

![innodb_20190318092942](/img/innodb_20190318092942.jpg)

![innodb_20190318093009](/img/innodb_20190318093009.jpg)

使用show engine innodb status查看死锁状态。先后出现lock_mode X locks gap before rec insert intention waiting和lock_mode X locks gap before rec字眼，是gap锁和插入意向锁的冲突导致的死锁。



#### **回顾select...for update的加锁范围**

首先回顾一下两个事务中的select ... for update做了哪些加锁操作：

code=5时，首先会获取code=5的索引记录锁(Record锁)，根据之前gap锁的介绍，会在前一个索引和当前索引之间的间隙加锁，于是区间(1,5)之间被加上了X模式的gap锁。除此之外RR模式下，还会加next-key锁，于是区间(5,10]被加了next-key锁。

- 因此，code=5的加锁范围是，区间(1,5)的gap锁，{5}索引Record锁，(5,10]的next-key锁。即区间(1,10)上都被加上了X模式的锁。
- 同理，code=10的加锁范围是，区间(5,10)的gap锁，{10}索引Record锁，（10,+∞)的next-key锁。

由gap锁的特性，兼容矩阵中冲突的锁也可以被不同的事务同时加在一个间隙上。上述两个select ... for update语句出现了间隙锁的交集，code=5的next-key锁和code=10的gap锁有重叠的区域——(5,10)。



#### **死锁的成因**

当事务一执行插入语句时，会先加X模式的插入意向锁，即兼容矩阵中的IX锁。但是由于插入意向锁要锁定的位置存在X模式的gap锁。兼容矩阵中IX和X锁是不兼容的，因此事务一的IX锁会等待事务二的gap锁释放。

事务二也执行插入语句，与事务一同样，事务二的插入意向锁IX锁会等待事务一的gap锁释放。

两个事务互相等待对方先释放锁，因此出现死锁。

**六、总结**

除了以上给出的几种死锁模式，还有很多其他死锁的场景。

无论是哪种场景，万变不离其宗，都是由于某个区间上或者某一个记录上可以同时持有锁，例如不同事务在同一个间隙gap上的锁不冲突；不同事务中，S锁可以阻塞X锁的获取，但是不会阻塞另一个事务获取该S锁。这样才会出现两个事务同时持有锁，并互相等待，最终导致死锁。

其中需要注意的点是，增、删、改的操作都会进行一次当前读操作，以此获取最新版本的数据，并检测是否有重复的索引。这个过程除了会导致RR隔离级别下出现死锁之外还会导致其他两个问题：

- 第一个是可重复读可能会因为这次的当前读操作而中断，(同样，幻读可能也会因此产生)；
- 第二个是其他事务的更新可能会丢失(解决方式：悲观锁、乐观锁)。







# 怎么保证缓存与数据库的双写一致性？



- 面试题剖析

- - Cache Aside Pattern
  - 最初级的缓存不一致问题及解决方案
  - 比较复杂的数据不一致问题分析

------

只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？

## 面试题剖析

一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。

串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上请求。

## Cache Aside Pattern

最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。

读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。

更新的时候，先更新数据库，然后再删除缓存。

为什么是删除缓存，而不是更新缓存？

原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？

举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低，用到缓存才去算缓存。

其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。

## 最初级的缓存不一致问题及解决方案

问题：先修改数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。

![img](/img/640.webp)

解决思路：先删除缓存，再修改数据库。如果数据库修改失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。

## 比较复杂的数据不一致问题分析

数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。

完了，数据库和缓存中的数据不一样了。。。

为什么上亿流量高并发场景下，缓存会出现这个问题？

只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。

**解决方案如下：**

更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。

一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。

待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。

如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。

高并发的场景下，该解决方案要注意的问题：

**1、读请求长时阻塞**

由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。

该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。

另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每隔库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。

一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。

如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。

其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。

实际粗略测算一下

如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。

经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。

**2、读请求并发量过高**

这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。

但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。

**3、多服务实例部署的请求路由**

可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。

比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。

**4、热点商品的路由问题，导致请求的倾斜**

万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。





# 秒杀系统架构

===================================

作者丨猿码道

jianshu.com/p/df4fbecb1a4b

===================================

## 1、秒杀业务分析

正常电子商务流程

（1）查询商品；

（2）创建订单；

（3）扣减库存；

（4）更新订单；

（5）付款；

（6）卖家发货；

秒杀业务的特性

（1）低廉价格；

（2）大幅推广；

（3）瞬时售空；

（4）一般是定时上架；

（5）时间短、瞬时并发量高；

## 2、秒杀技术挑战

假设某网站秒杀活动只推出一件商品，预计会吸引1万人参加活动，也就说最大并发请求数是10000，秒杀系统需要面对的技术挑战有：

**1.对现有网站业务造成冲击**

秒杀活动只是网站营销的一个附加活动，这个活动具有时间短，并发访问量大的特点

如果和网站原有应用部署在一起，必然会对现有业务造成冲击，稍有不慎可能导致整个网站瘫痪。

> 解决方案：将秒杀系统独立部署，甚至使用独立域名，使其与网站完全隔离。

**2.高并发下的应用、数据库负载**

用户在秒杀开始前，通过不停刷新浏览器页面以保证不会错过秒杀，这些请求如果按照一般的网站应用架构，访问应用服务器、连接数据库，会对应用服务器和数据库服务器造成负载压力。

> 解决方案：重新设计秒杀商品页面，不使用网站原来的商品详细页面，页面内容静态化，用户请求不需要经过应用服务。

**3.突然增加的网络及服务器带宽**

假设商品页面大小200K（主要是商品图片大小），那么需要的网络和服务器带宽是2G（200K×10000）

这些网络带宽是因为秒杀活动新增的，超过网站平时使用的带宽。

> 解决方案：因为秒杀新增的网络带宽，必须和运营商重新购买或者租借。
>
> 为了减轻网站服务器的压力，需要将秒杀商品页面缓存在CDN，同样需要和CDN服务商临时租借新增的出口带宽。

**4.直接下单**

秒杀的游戏规则是到了秒杀才能开始对商品下单购买，在此时间点之前，只能浏览商品信息，不能下单。

而下单页面也是一个普通的URL，如果得到这个URL，不用等到秒杀开始就可以下单了。

> 解决方案：为了避免用户直接访问下单页面URL，需要将改URL动态化，即使秒杀系统的开发者也无法在秒杀开始前访问下单页面的URL。
>
> 办法是在下单页面URL加入由服务器端生成的随机数作为参数，在秒杀开始的时候才能得到。

**5.如何控制秒杀商品页面购买按钮的点亮**

购买按钮只有在秒杀开始的时候才能点亮，在此之前是灰色的。

如果该页面是动态生成的，当然可以在服务器端构造响应页面输出，控制该按钮是灰色还是点亮

但是为了减轻服务器端负载压力，更好地利用CDN、反向代理等性能优化手段，该页面被设计为静态页面，缓存在CDN、反向代理服务器上，甚至用户浏览器上。

秒杀开始时，用户刷新页面，请求根本不会到达应用服务器。

> 解决方案：使用JavaScript脚本控制，在秒杀商品静态页面中加入一个JavaScript文件引用，该JavaScript文件中包含秒杀开始标志为否；
>
> 当秒杀开始的时候生成一个新的JavaScript文件（文件名保持不变，只是内容不一样），更新秒杀开始标志为是，加入下单页面的URL及随机数参数
>
> （这个随机数只会产生一个，即所有人看到的URL都是同一个，服务器端可以用redis这种分布式缓存服务器来保存随机数）
>
> 然后被用户浏览器加载，控制秒杀商品页面的展示。这个JavaScript文件的加载可以加上随机版本号（例如xx.js?v=32353823），这样就不会被浏览器、CDN和反向代理服务器缓存。
>
> 这个JavaScript文件非常小，即使每次浏览器刷新都访问JavaScript文件服务器也不会对服务器集群和网络带宽造成太大压力。

**6.如何只允许第一个提交的订单被发送到订单子系统**

由于最终能够成功秒杀到商品的用户只有一个，因此需要在用户提交订单时，检查是否已经有订单提交。

如果已经有订单提交成功，则需要更新 JavaScript文件，更新秒杀开始标志为否，购买按钮变灰。

事实上，由于最终能够成功提交订单的用户只有一个，为了减轻下单页面服务器的负载压力，可以控制进入下单页面的入口，只有少数用户能进入下单页面，其他用户直接进入秒杀结束页面。

> 解决方案：假设下单服务器集群有10台服务器，每台服务器只接受最多10个下单请求。
>
> 在还没有人提交订单成功之前，如果一台服务器已经有十单了，而有的一单都没处理，可能出现的用户体验不佳的场景是用户第一次点击购买按钮进入已结束页面，再刷新一下页面，有可能被一单都没有处理的服务器处理，进入了填写订单的页面，可以考虑通过cookie的方式来应对，符合一致性原则。
>
> 当然可以采用最少连接的负载均衡算法，出现上述情况的概率大大降低。

**7.如何进行下单前置检查**

下单服务器检查本机已处理的下单请求数目：

- 如果超过10条，直接返回已结束页面给用户；
- 如果未超过10条，则用户可进入填写订单及确认页面；

检查全局已提交订单数目：

- 已超过秒杀商品总数，返回已结束页面给用户；
- 未超过秒杀商品总数，提交到子订单系统；

**8.秒杀一般是定时上架**

该功能实现方式很多。不过目前比较好的方式是：提前设定好商品的上架时间，用户可以在前台看到该商品，但是无法点击“立即购买”的按钮。

但是需要考虑的是，有人可以绕过前端的限制，直接通过URL的方式发起购买，这就需要在前台商品页面，以及bug页面到后端的数据库，都要进行时钟同步。越在后端控制，安全性越高。

定时秒杀的话，就要避免卖家在秒杀前对商品做编辑带来的不可预期的影响。这种特殊的变更需要多方面评估。一般禁止编辑，如需变更，可以走数据订正的流程。

**9.减库存的操作**

有两种选择，一种是拍下减库存 另外一种是付款减库存；目前采用的“拍下减库存”的方式，拍下就是一瞬间的事，对用户体验会好些。

**10.库存会带来“超卖”的问题：售出数量多于库存数量**

由于库存并发更新的问题，导致在实际库存已经不足的情况下，库存依然在减，导致卖家的商品卖得件数超过秒杀的预期。方案：采用乐观锁

```
update auction_auctions setquantity = #inQuantity#where auction_id = #itemId# and quantity = #dbQuantity#
```

还有一种方式，会更好些，叫做尝试扣减库存，扣减库存成功才会进行下单逻辑：

```
update auction_auctions set quantity = quantity-#count# where auction_id = #itemId# and quantity >= #count# 
```

**11.秒杀器的应对**

秒杀器一般下单个购买及其迅速，根据购买记录可以甄别出一部分。可以通过校验码达到一定的方法，这就要求校验码足够安全，不被破解，采用的方式有：秒杀专用验证码，电视公布验证码，秒杀答题。

## 3、秒杀架构原则

**1.尽量将请求拦截在系统上游**

传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小

【一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0】

**2.读多写少的常用多使用缓存**

这是一个典型的读多写少的应用场景【一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%】，非常适合使用缓存。

## 4、秒杀架构设计

秒杀系统为秒杀而设计，不同于一般的网购行为，参与秒杀活动的用户更关心的是如何能快速刷新商品页面

在秒杀开始的时候抢先进入下单页面，而不是商品详情等用户体验细节，因此秒杀系统的页面设计应尽可能简单。

商品页面中的购买按钮只有在秒杀活动开始的时候才变亮，在此之前及秒杀商品卖出后，该按钮都是灰色的，不可以点击。

下单表单也尽可能简单，购买数量只能是一个且不可以修改，送货地址和付款方式都使用用户默认设置，没有默认也可以不填，允许等订单提交后修改；

只有第一个提交的订单发送给网站的订单子系统，其余用户提交订单后只能看到秒杀结束页面。

**要做一个这样的秒杀系统，业务会分为两个阶段：**

- 第一个阶段是秒杀开始前某个时间到秒杀开始， 这个阶段可以称之为准备阶段，用户在准备阶段等待秒杀；
- 第二个阶段就是秒杀开始到所有参与秒杀的用户获得秒杀结果， 这个就称为秒杀阶段吧。

4.1 前端层设计

首先要有一个展示秒杀商品的页面，在这个页面上做一个秒杀活动开始的倒计时，在准备阶段内用户会陆续打开这个秒杀的页面， 并且可能不停的刷新页面。这里需要考虑两个问题：

**第一个是秒杀页面的展示**

我们知道一个html页面还是比较大的，即使做了压缩，http头和内容的大小也可能高达数十K

加上其他的css， js，图片等资源，如果同时有几千万人参与一个商品的抢购，一般机房带宽也就只有1G10G

**网络带宽就极有可能成为瓶颈，所以这个页面上各类静态资源首先应分开存放，然后放到cdn节点上分散压力**

由于CDN节点遍布全国各地，能缓冲掉绝大部分的压力，而且还比机房带宽便宜

**第二个是倒计时**

出于性能原因这个一般由js调用客户端本地时间，就有可能出现客户端时钟与服务器时钟不一致，另外服务器之间也是有可能出现时钟不一致。

客户端与服务器时钟不一致可以采用客户端定时和服务器同步时间。

这里考虑一下性能问题，用于同步时间的接口由于不涉及到后端逻辑，只需要将当前web服务器的时间发送给客户端就可以了，因此速度很快

就我以前测试的结果来看，一台标准的web服务器2W+QPS不会有问题，如果100W人同时刷，100W QPS也只需要50台web，一台硬件LB就可以了~。

并且web服务器群是可以很容易的横向扩展的(LB+DNS轮询)，这个接口可以只返回一小段json格式的数据

而且可以优化一下减少不必要cookie和其他http头的信息，所以数据量不会很大

一般来说网络不会成为瓶颈，即使成为瓶颈也可以考虑多机房专线连通，加智能DNS的解决方案；

web服务器之间时间不同步可以采用统一时间服务器的方式，比如每隔1分钟所有参与秒杀活动的web服务器就与时间服务器做一次时间同步。

**浏览器层请求拦截**

- 产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求;
- JS层面，限制用户在x秒之内只能提交一次请求;

4.2 站点层设计

前端层的请求拦截，只能拦住小白用户（不过这是99%的用户哟），高端的程序员根本不吃这一套，写个for循环，直接调用你后端的http请求，怎么整？

- 同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面
- 同一个item的查询，例如手机车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面

如此限流，又有99%的流量会被拦截在站点层。

4.3 服务层设计

站点层的请求拦截，只能拦住普通程序员，高级黑客，假设他控制了10w台肉鸡（并且假设买票不需要实名认证），这下uid的限制不行了吧？怎么整？

- 大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？
- 对于写请求，做请求队列，每次只透过有限的写请求去数据层，如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”；
- 对于读请求，还用说么？cache来抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的；

如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。

- **用户请求分发模块：**使用Nginx或Apache将用户的请求分发到不同的机器上。
- **用户请求预处理模块：**判断商品是不是还有剩余来决定是不是要处理该请求。
- **用户请求处理模块：**把通过预处理的请求封装成事务提交给数据库，并返回是否成功。
- **数据库接口模块：**该模块是数据库的唯一接口，负责与数据库交互，提供RPC接口供查询是否秒杀结束、剩余数量等信息。

**用户请求预处理模块**

经过HTTP服务器的分发后，单个服务器的负载相对低了一些，但总量依然可能很大

如果后台商品已经被秒杀完毕，那么直接给后来的请求返回秒杀失败即可，不必再进一步发送事务了

示例代码可以如下所示：

```
package seckill;import org.apache.http.HttpRequest;/** * 预处理阶段，把不必要的请求直接驳回，必要的请求添加到队列中进入下一阶段. */public class PreProcessor { // 商品是否还有剩余 private static boolean reminds = true; private static void forbidden() { // Do something. } public static boolean checkReminds() { if (reminds) { // 远程检测是否还有剩余，该RPC接口应由数据库服务器提供，不必完全严格检查. if (!RPC.checkReminds()) { reminds = false; } } return reminds; } /** * 每一个HTTP请求都要经过该预处理. */ public static void preProcess(HttpRequest request) { if (checkReminds()) { // 一个并发的队列 RequestQueue.queue.add(request); } else { // 如果已经没有商品了，则直接驳回请求即可. forbidden(); } }}
```

**并发队列的选择**

Java的并发包提供了三个常用的并发队列实现，分别是：

ConcurrentLinkedQueue

LinkedBlockingQueue

ArrayBlockingQueue

- ArrayBlockingQueue是初始容量固定的阻塞队列，我们可以用来作为数据库模块成功竞拍的队列，比如有10个商品，那么我们就设定一个10大小的数组队列。
- ConcurrentLinkedQueue使用的是CAS原语无锁队列实现，是一个异步队列，入队的速度很快，出队进行了加锁，性能稍慢。
- LinkedBlockingQueue也是阻塞的队列，入队和出队都用了加锁，当队空的时候线程会暂时阻塞。

由于我们的系统入队需求要远大于出队需求，一般不会出现队空的情况，所以我们可以选择ConcurrentLinkedQueue来作为我们的请求队列实现：

```
package seckill;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ConcurrentLinkedQueue;import org.apache.http.HttpRequest;public class RequestQueue { public static ConcurrentLinkedQueue queue = new ConcurrentLinkedQueue();}
```

**用户请求模块**

```
package seckill;import org.apache.http.HttpRequest;public class Processor { /** * 发送秒杀事务到数据库队列. */ public static void kill(BidInfo info) { DB.bids.add(info); } public static void process() { BidInfo info = new BidInfo(RequestQueue.queue.poll()); if (info != null) { kill(info); } }}class BidInfo { BidInfo(HttpRequest request) { // Do something. }}
```

**数据库模块**

数据库主要是使用一个ArrayBlockingQueue来暂存有可能成功的用户请求。

```
package seckill;import java.util.concurrent.ArrayBlockingQueue;/** * DB应该是数据库的唯一接口. */public class DB { public static int count = 10; public static ArrayBlockingQueue bids = new ArrayBlockingQueue(10); public static boolean checkReminds() { // TODO return true; } // 单线程操作 public static void bid() { BidInfo info = bids.poll(); while (count-- > 0) { // insert into table Bids values(item_id, user_id, bid_date, other) // select count(id) from Bids where item_id = ? // 如果数据库商品数量大约总数，则标志秒杀已完成，设置标志位reminds = false. info = bids.poll(); } }}
```

4.4 数据库设计

4.4.1 基本概念

**概念一“单库”**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/8c0967fb08c9448dbd4040d95a4ff87c.jpg)



**概念二“分片”**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/32020becdf70409394bba545650574d0.jpg)



分片解决的是“数据量太大”的问题，也就是通常说的“水平切分”。

一旦引入分片，势必有“数据路由”的概念，哪个数据访问哪个库。路由规则通常有3种方法：

**1、范围：range**

优点：简单，容易扩展

缺点：各库压力不均（新号段更活跃）

**2、哈希：hash 【大部分互联网公司采用的方案二：哈希分库，哈希路由】**

优点：简单，数据均衡，负载均匀

缺点：迁移麻烦（2库扩3库数据要迁移）

**3、路由服务：router-config-server**

优点：灵活性强，业务与路由算法解耦

缺点：每次访问数据库前多一次查询

**概念三“分组”**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/1a7aad415c4c4b5faddd1923a634dfe7.jpg)



分组解决“可用性”问题，分组通常通过主从复制的方式实现。

**互联网公司数据库实际软件架构是：又分片，又分组（如下图）**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/ff5f7e71b280416cadf137bc12299f0d.jpg)



4.4.2 设计思路

数据库软件架构师平时设计些什么东西呢？至少要考虑以下四点：

- 如何保证数据可用性；
- 如何提高数据库读性能（大部分应用读多写少，读会先成为瓶颈）；
- 如何保证一致性；
- 如何提高扩展性；

**1.如何保证数据的可用性？**

解决可用性问题的思路是=>冗余

> 如何保证站点的可用性？复制站点，冗余站点
>
> 如何保证服务的可用性？复制服务，冗余服务
>
> 如何保证数据的可用性？复制数据，冗余数据

数据的冗余，会带来一个副作用=>引发一致性问题（先不说一致性问题，先说可用性）。

**2.如何保证数据库“读”高可用？**

冗余读库

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/c53637053a1b4e92b9a4a7d9adb775d5.jpg)



冗余读库带来的副作用？

读写有延时，可能不一致。

上面这个图是很多互联网公司mysql的架构，写仍然是单点，不能保证写高可用。

**3.如何保证数据库“写”高可用？**

冗余写库

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/6cbc97c3cca14d47a46d55bfc1a52828.jpg)



采用双主互备的方式，可以冗余写库带来的副作用？双写同步，数据可能冲突（例如“自增id”同步冲突），如何解决同步冲突，有两种常见解决方案：

- 两个写库使用不同的初始值，相同的步长来增加id：1写库的id为0,2,4,6…；2写库的id为1,3,5,7…；

- 不使用数据的id，业务层自己生成唯一的id，保证数据不冲突；

实际中没有使用上述两种架构来做读写的“高可用”，采用的是“双主当主从用”的方式：

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/10d93519a0a349ceaaa49a3835d7b351.jpg)



仍是双主，但只有一个主提供服务（读+写），另一个主是“shadow-master”，只用来保证高可用，平时不提供服务。

master挂了，shadow-master顶上（vip漂移，对业务层透明，不需要人工介入）。

**这种方式的好处：**

- 读写没有延时；
- 读写高可用；

**不足：**

- 不能通过加从库的方式扩展读性能；
- 资源利用率为50%，一台冗余主没有提供服务；

那如何提高读性能呢？进入第二个话题，如何提供读性能。

**4.如何扩展读性能**

提高读性能的方式大致有三种：

第一种是建立索引。这种方式不展开，要提到的一点是，不同的库可以建立不同的索引。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/b96e1c20903443d698d88f1a0a1f8abe.jpg)



> 写库不建立索引；
>
> 线上读库建立线上访问索引，例如uid；
>
> 线下读库建立线下访问索引，例如time；

第二种扩充读性能的方式是，增加从库，这种方法大家用的比较多，但是，存在两个缺点：

- 从库越多，同步越慢；
- 同步越慢，数据不一致窗口越大（不一致后面说，还是先说读性能的提高）；

实际中没有采用这种方法提高数据库读性能（没有从库），采用的是增加缓存。常见的缓存架构如下：

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/26969d085abe4081821617dbc4ce234d.jpg)



上游是业务应用，下游是主库，从库（读写分离），缓存。实际的玩法：服务+数据库+缓存一套。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/1076e55b1c1d451f8ca0b74ca0ab70df.jpg)



业务层不直接面向db和cache，服务层屏蔽了底层db、cache的复杂性。

为什么要引入服务层，今天不展开，采用了“服务+数据库+缓存一套”的方式提供数据访问，用cache提高读性能。

不管采用主从的方式扩展读性能，还是缓存的方式扩展读性能，数据都要复制多份（主+从，db+cache），一定会引发一致性问题。

## **5.如何保证一致性？**

主从数据库的一致性，通常有两种解决方案：

**1、中间件**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/9e108b130e514d8fb41bae4bb5bda5b2.jpg)



如果某一个key有写操作，在不一致时间窗口内，中间件会将这个key的读操作也路由到主库上。

这个方案的缺点是，数据库中间件的门槛较高（百度，腾讯，阿里，360等一些公司有）

**2、强制读主**

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/3c0f26d4194d41b5b6c1ffcaee4e6aed.jpg)



上面实际用的“双主当主从用”的架构，不存在主从不一致的问题。第二类不一致，是db与缓存间的不一致：

![这是我读过写得最好的「秒杀系统架构」分析与实战](http://p3.pstatp.com/large/pgc-image/26969d085abe4081821617dbc4ce234d)



常见的缓存架构如上，此时写操作的顺序是：

（1）淘汰cache；

（2）写数据库；

读操作的顺序是：

（1）读cache，如果cache hit则返回；

（2）如果cache miss，则读从库；

（3）读从库后，将数据放回cache；

在一些异常时序情况下，有可能从【从库读到旧数据（同步还没有完成），旧数据入cache后】，数据会长期不一致。解决办法是“缓存双淘汰”，写操作时序升级为：

（1）淘汰cache；

（2）写数据库；

（3）在经过“主从同步延时窗口时间”后，再次发起一个异步淘汰cache的请求；

这样，即使有脏数据如cache，一个小的时间窗口之后，脏数据还是会被淘汰。带来的代价是，多引入一次读miss（成本可以忽略）。

除此之外，最佳实践之一是：建议为所有cache中的item设置一个超时时间。

**3.如何提高数据库的扩展性？**

原来用hash的方式路由，分为2个库，数据量还是太大，要分为3个库，势必需要进行数据迁移，有一个很帅气的“数据库秒级扩容”方案。

**如何秒级扩容？**

首先，我们不做2库变3库的扩容，我们做2库变4库（库加倍）的扩容（未来4->8->16）

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/3dfb30f421604111ad5a170ab9ab87e0.jpg)



服务+数据库是一套（省去了缓存），数据库采用“双主”的模式。

扩容步骤：

- 第一步，将一个主库提升;
- 第二步，修改配置，2库变4库（原来MOD2，现在配置修改后MOD4），扩容完成；

原MOD2为偶的部分，现在会MOD4余0或者2；原MOD2为奇的部分，现在会MOD4余1或者3；数据不需要迁移

同时，双主互相同步，一遍是余0，一边余2，两边数据同步也不会冲突，秒级完成扩容！

最后，要做一些收尾工作：

- 将旧的双主同步解除；
- 增加新的双主（双主是保证可用性的，shadow-master平时不提供服务）；
- 删除多余的数据（余0的主，可以将余2的数据删除掉）；

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/c8414d0f2e014909934fbf10e78cee48.jpg)



这样，秒级别内，我们就完成了2库变4库的扩展。

5、大并发带来的挑战

**5.1、请求接口的合理设计**

一个秒杀或者抢购页面，通常分为2个部分，一个是静态的HTML等内容，另一个就是参与秒杀的Web后台请求接口。

通常静态HTML等内容，是通过CDN的部署，一般压力不大，核心瓶颈实际上在后台请求接口上。

这个后端接口，必须能够支持高并发请求，同时，非常重要的一点，必须尽可能“快”，在最短的时间里返回用户的请求结果。

为了实现尽可能快这一点，接口的后端存储使用内存级别的操作会更好一点。

仍然直接面向MySQL之类的存储是不合适的，如果有这种复杂业务的需求，都建议采用异步写入。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/305170add74e4daf9d4745ec5883c431.jpg)



当然，也有一些秒杀和抢购采用“滞后反馈”，就是说秒杀当下不知道结果，一段时间后才可以从页面中看到用户是否秒杀成功。

但是，这种属于“偷懒”行为，同时给用户的体验也不好，容易被用户认为是“暗箱操作”。

**5.2 高并发的挑战：一定要“快”**

我们通常衡量一个Web系统的吞吐率的指标是QPS（Query Per Second，每秒处理请求数）

解决每秒数万次的高并发场景，这个指标非常关键。

举个例子，我们假设处理一个业务请求平均响应时间为100ms，同时，系统内有20台Apache的Web服务器，配置MaxClients为500个（表示Apache的最大连接数目）。

那么，我们的Web系统的理论峰值QPS为（理想化的计算方式）：

> 20*500/0.1 = 100000 （10万QPS）

咦？我们的系统似乎很强大，1秒钟可以处理完10万的请求，5w/s的秒杀似乎是“纸老虎”哈。

实际情况，当然没有这么理想。在高并发的实际场景下，机器都处于高负载的状态，在这个时候平均响应时间会被大大增加。

就Web服务器而言，Apache打开了越多的连接进程，CPU需要处理的上下文切换也越多，额外增加了CPU的消耗，然后就直接导致平均响应时间增加。

因此上述的MaxClient数目，要根据CPU、内存等硬件因素综合考虑，绝对不是越多越好。可以通过Apache自带的abench来测试一下，取一个合适的值。

然后，我们选择内存操作级别的存储的Redis，在高并发的状态下，存储的响应时间至关重要。

网络带宽虽然也是一个因素，不过，这种请求数据包一般比较小，一般很少成为请求的瓶颈。负载均衡成为系统瓶颈的情况比较少，在这里不做讨论哈。

那么问题来了，假设我们的系统，在5w/s的高并发状态下，平均响应时间从100ms变为250ms（实际情况，甚至更多）：

> 20*500/0.25 = 40000 （4万QPS）

于是，我们的系统剩下了4w的QPS，面对5w每秒的请求，中间相差了1w。

然后，这才是真正的恶梦开始。

举个例子，高速路口，1秒钟来5部车，每秒通过5部车，高速路口运作正常。突然，这个路口1秒钟只能通过4部车，车流量仍然依旧，结果必定出现大塞车。（5条车道忽然变成4条车道的感觉）。

同理，某一个秒内，20*500个可用连接进程都在满负荷工作中，却仍然有1万个新来请求，没有连接进程可用，系统陷入到异常状态也是预期之内。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/5ab12a9e8aa548ea9048e99dd994cd33.jpg)



其实在正常的非高并发的业务场景中，也有类似的情况出现

某个业务请求接口出现问题，响应时间极慢，将整个Web请求响应时间拉得很长，逐渐将Web服务器的可用连接数占满，其他正常的业务请求，无连接进程可用。

更可怕的问题是，是用户的行为特点，系统越是不可用，用户的点击越频繁，恶性循环最终导致“雪崩”

（其中一台Web机器挂了，导致流量分散到其他正常工作的机器上，再导致正常的机器也挂，然后恶性循环），将整个Web系统拖垮。

**5.3 重启与过载保护**

如果系统发生“雪崩”，贸然重启服务，是无法解决问题的。

最常见的现象是，启动起来后，立刻挂掉。这个时候，最好在入口层将流量拒绝，然后再将重启。

如果是redis/memcache这种服务也挂了，重启的时候需要注意“预热”，并且很可能需要比较长的时间。

秒杀和抢购的场景，流量往往是超乎我们系统的准备和想象的。

这个时候，过载保护是必要的。如果检测到系统满负载状态，拒绝请求也是一种保护措施。

在前端设置过滤是最简单的方式，但是，这种做法是被用户“千夫所指”的行为。

更合适一点的是，将过载保护设置在CGI入口层，快速将客户的直接请求返回。

## 6、作弊的手段：进攻与防守

秒杀和抢购收到了“海量”的请求，实际上里面的水分是很大的。

不少用户，为了“抢“到商品，会使用“刷票工具”等类型的辅助工具，帮助他们发送尽可能多的请求到服务器。

还有一部分高级用户，制作强大的自动请求脚本。这种做法的理由也很简单，就是在参与秒杀和抢购的请求中，自己的请求数目占比越多，成功的概率越高。

这些都是属于“作弊的手段”，不过，有“进攻”就有“防守”，这是一场没有硝烟的战斗哈。

**6.1、同一个账号，一次性发出多个请求**

部分用户通过浏览器的插件或者其他工具，在秒杀开始的时间里，以自己的账号，一次发送上百甚至更多的请求。实际上，这样的用户破坏了秒杀和抢购的公平性。

这种请求在某些没有做数据安全处理的系统里，也可能造成另外一种破坏，导致某些判断条件被绕过。

例如一个简单的领取逻辑，先判断用户是否有参与记录，如果没有则领取成功，最后写入到参与记录中。

这是个非常简单的逻辑，但是，在高并发的场景下，存在深深的漏洞。

多个并发请求通过负载均衡服务器，分配到内网的多台Web服务器，它们首先向存储发送查询请求

然后，在某个请求成功写入参与记录的时间差内，其他的请求获查询到的结果都是“没有参与记录”。这里，就存在逻辑判断被绕过的风险。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/8b8c744625b74da6bd46edea181f7a72.jpg)



**应对方案：**

在程序入口处，一个账号只允许接受1个请求，其他请求过滤。

这样不仅解决了同一个账号，发送N个请求的问题，还保证了后续的逻辑流程的安全。

实现方案，可以通过Redis这种内存缓存服务，写入一个标志位（只允许1个请求写成功，结合watch的乐观锁的特性），成功写入的则可以继续参加。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/c2d3f6e15cdf4fdea8b5a84f5d6653e1.jpg)



或者，自己实现一个服务，将同一个账号的请求放入一个队列中，处理完一个，再处理下一个。

**6.2、多个账号，一次性发送多个请求**

很多公司的账号注册功能，在发展早期几乎是没有限制的，很容易就可以注册很多个账号。

因此，也导致了出现了一些特殊的工作室，通过编写自动注册脚本，积累了一大批“僵尸账号”，数量庞大，几万甚至几十万的账号不等，专门做各种刷的行为（这就是微博中的“僵尸粉“的来源）。

举个例子，例如微博中有转发抽奖的活动，如果我们使用几万个“僵尸号”去混进去转发，这样就可以大大提升我们中奖的概率。

这种账号，使用在秒杀和抢购里，也是同一个道理。例如，iPhone官网的抢购，火车票黄牛党。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/e9bf3c66dea440198760449426e424cd.jpg)



**应对方案：**

这种场景，可以通过检测指定机器IP请求频率就可以解决，如果发现某个IP请求频率很高，可以给它弹出一个验证码或者直接禁止它的请求：

弹出验证码，最核心的追求，就是分辨出真实用户。

因此，大家可能经常发现，网站弹出的验证码，有些是“鬼神乱舞”的样子，有时让我们根本无法看清。

他们这样做的原因，其实也是为了让验证码的图片不被轻易识别，因为强大的“自动脚本”可以通过图片识别里面的字符，然后让脚本自动填写验证码。

实际上，有一些非常创新的验证码，效果会比较好，例如给你一个简单问题让你回答，或者让你完成某些简单操作（例如百度贴吧的验证码）。

直接禁止IP，实际上是有些粗暴的，因为有些真实用户的网络场景恰好是同一出口IP的，可能会有“误伤“。

但是这一个做法简单高效，根据实际场景使用可以获得很好的效果。

6.3、多个账号，不同IP发送不同请求

所谓道高一尺，魔高一丈。有进攻，就会有防守，永不休止。

这些“工作室”，发现你对单机IP请求频率有控制之后，他们也针对这种场景，想出了他们的“新进攻方案”，就是不断改变IP。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/e12594592c804b7385fa6fcd899576af.jpg)



有同学会好奇，这些随机IP服务怎么来的。

有一些是某些机构自己占据一批独立IP，然后做成一个随机代理IP的服务，有偿提供给这些“工作室”使用。

还有一些更为黑暗一点的，就是通过木马黑掉普通用户的电脑，这个木马也不破坏用户电脑的正常运作，只做一件事情，就是转发IP包，普通用户的电脑被变成了IP代理出口。

通过这种做法，黑客就拿到了大量的独立IP，然后搭建为随机IP服务，就是为了挣钱。

**应对方案：**

说实话，这种场景下的请求，和真实用户的行为，已经基本相同了，想做分辨很困难。再做进一步的限制很容易“误伤“真实用户

这个时候，通常只能通过设置业务门槛高来限制这种请求了，或者通过账号行为的”数据挖掘“来提前清理掉它们。

僵尸账号也还是有一些共同特征的，例如账号很可能属于同一个号码段甚至是连号的，活跃度不高，等级低，资料不全等等。

根据这些特点，适当设置参与门槛，例如限制参与秒杀的账号等级。通过这些业务手段，也是可以过滤掉一些僵尸号。

## 7、高并发下的数据安全

我们知道在多线程写入同一个文件的时候，会存现“线程安全”的问题

（多个线程同时运行同一段代码，如果每次运行结果和单线程运行的结果是一样的，结果和预期相同，就是线程安全的）

如果是MySQL数据库，可以使用它自带的锁机制很好的解决问题，但是，在大规模并发的场景中，是不推荐使用MySQL的。

秒杀和抢购的场景中，还有另外一个问题，就是“超发”，如果在这方面控制不慎，会产生发送过多的情况。

我们也曾经听说过，某些电商搞抢购活动，买家成功拍下后，商家却不承认订单有效，拒绝发货。

这里的问题，也许并不一定是商家奸诈，而是系统技术层面存在超发风险导致的。

**7.1、超发的原因**

假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。

这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是99个，然后都通过了这一个余量判断，最终导致超发。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/2caab14ac1e44ee1a227f727b7b71d2d.jpg)



在上面的这个图中，就导致了并发用户B也“抢购成功”，多让一个人获得了商品。这种场景，在高并发的情况下非常容易出现。

**7.2、悲观锁思路**

解决线程安全的思路很多，可以从“悲观锁”的方向开始讨论。

悲观锁，也就是在修改数据的时候，采用锁定状态，排斥外部请求的修改。遇到加锁的状态，就必须等待。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/f34efde945a148339378128c333c2023.jpg)



虽然上述的方案的确解决了线程安全的问题，但是，别忘记，我们的场景是“高并发”。

也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。

同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。

**7.3、FIFO队列思路**

那好，那么我们稍微修改一下上面的场景，我们直接将请求放入队列中的，采用FIFO（First Input First Output，先进先出）

这样的话，我们就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/f01ea7e1d9ea4f49aa330ebea4e64704.jpg)



然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。

那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。

或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。

也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。

**7.4、乐观锁思路**

这个时候，我们就可以讨论一下“乐观锁”的思路了。

乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。

实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。

这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。

![这是我读过写得最好的「秒杀系统架构」分析与实战](/img/84a4bf2a5b744929aa78131060b548a8.jpg)



有很多软件和服务都“乐观锁”功能的支持，例如Redis中的watch就是其中之一。通过这个实现，我们保证了数据的安全。

## 8、总结

互联网正在高速发展，使用互联网服务的用户越多，高并发的场景也变得越来越多。

电商秒杀和抢购，是两个比较典型的互联网高并发场景。虽然我们解决问题的具体技术方案可能千差万别，但是遇到的挑战却是相似的，因此解决问题的思路也异曲同工。

**END**



# Java锁机制之ReentrantLock实现原理浅析

## **摘要**

ReentrantLock是Java中经常用到的锁之一，在实际生产中我们经常通过ReentrantLock来保证线程同步，那么其底层是如何实现锁机制的？这一点也是面试中面试官经常会考察到的，本文对ReentrantLock进行初步原理解析。

## **ReentrantLock概述**

经典的ReentrantLock使用方法如下：

```
private Lock lock = new ReentrantLock();public void test(){ lock.lock(); try{ doSomeThing(); }catch (Exception e){ // ignored }finally { lock.unlock(); }}
```

ReentrantLock主要利用CAS+AQS队列来实现。它支持公平锁和非公平锁，两者的实现类似。

**[CAS]**：Compare and Swap，比较并交换。CAS有3个操作数：内存值V、预期值A、要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。该操作是一个**原子操作**，被广泛的应用在Java的底层实现中。在Java中，CAS主要是由sun.misc.Unsafe这个类**通过JNI调用CPU底层指令实现**，无论哪种情况，它都会在 CAS 指令之前返回该位置的值。CAS 有效地说明了“我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。” Java并发包(java.util.concurrent)中大量使用了CAS操作,涉及到并发的地方都调用了sun.misc.Unsafe类方法进行CAS操作。

**[AQS]**:Abstract Queued Synchronizer（摘要排队同步器）简称AQS，是一个用于构建锁和同步容器的框架。事实上concurrent包内许多类都是基于AQS构建，例如ReentrantLock，Semaphore，CountDownLatch，ReentrantReadWriteLock，FutureTask等。AQS解决了在实现同步容器时设计的大量细节问题。

![Java锁机制之ReentrantLock实现原理浅析](/img/942f16ff4ed64c8ca94b3e432138cb6a.jpg)



image-20190319140846163

AQS使用一个**FIFO的队列**表示排队等待锁的线程，队列**头节点称作“哨兵节点”或者“哑节点”**，它不与任何线程关联。其他的节点与等待线程关联，每个节点维护一个等待状态waitStatus

ReentrantLock的基本实现可以概括为：**先通过CAS尝试获取锁。如果此时已经有线程占据了锁，那就加入AQS队列并且被挂起。当锁被释放之后，排在CLH队列队首的线程会被唤醒，然后CAS再次尝试获取锁。在这个时候，如果：**

**非公平锁：如果同时还有另一个线程进来尝试获取，那么有可能会让这个线程抢先获取；**

**公平锁：如果同时还有另一个线程进来尝试获取，当它发现自己不是在队首的话，就会排到队尾，由队首的线程获取到锁。**

## **lock()与unlock()实现原理**

【**可重入锁**】：可重入锁是指同一个线程可以多次获取同一把锁。ReentrantLock和synchronized都是可重入锁。

【**可中断锁**】：可中断锁是指线程尝试获取锁的过程中，是否可以响应中断。**synchronized是不可中断锁，而ReentrantLock则提供了中断功能**。

【**公平锁与非公平锁**】：公平锁是指多个线程同时尝试获取同一把锁时，获取锁的顺序按照线程达到的顺序，而非公平锁则允许线程“插队”。synchronized是非公平锁，而ReentrantLock的默认实现是非公平锁，但是也可以设置为公平锁。

ReentrantLock提供了两个构造器，分别是

```
public ReentrantLock() { sync = new NonfairSync();}public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync();}
```

默认构造器初始化为NonfairSync对象，即非公平锁，而带参数的构造器可以指定使用公平锁和非公平锁。由lock()和unlock()的源码可以看到，它们只是分别调用了sync对象的lock()和release(1)方法。

**Lock（）**

```
final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);}
```

首先用一个CAS操作，判断state是否是0（表示当前锁未被占用），如果是0则把它置为1，并且设置当前线程为该锁的独占线程，表示获取锁成功。当多个线程同时尝试占用同一个锁时，CAS操作只能保证一个线程操作成功，剩下的只能乖乖的去排队啦。

 “非公平”即体现在这里，如果占用锁的线程刚释放锁，state置为0，而排队等待锁的线程还未唤醒时，新来的线程就直接抢占了该锁，那么就“插队”了。

 若当前有三个线程去竞争锁，假设线程A的CAS操作成功了，拿到了锁开开心心的返回了，那么线程B和C则设置state失败，走到了else里面。我们往下看acquire。

```
public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();}
```

1. 第一步。尝试去获取锁。如果尝试获取锁成功，方法直接返回。

```
tryAcquire(arg)final boolean nonfairTryAcquire(int acquires) { //获取当前线程 final Thread current = Thread.currentThread(); //获取state变量值 int c = getState(); if (c == 0) { //没有线程占用锁 if (compareAndSetState(0, acquires)) { //占用锁成功,设置独占线程为当前线程 setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { //当前线程已经占用该锁 int nextc = c + acquires; if (nextc < 0) // overflow throw new Error("Maximum lock count exceeded"); // 更新state值为新的重入次数 setState(nextc); return true; } //获取锁失败 return false;}
```

非公平锁tryAcquire的流程是：检查state字段，若为0，表示锁未被占用，那么尝试占用，若不为0，**检查当前锁是否被自己占用，若被自己占用，则更新state字段，表示重入锁的次数**。如果以上两点都没有成功，则获取锁失败，返回false。

1. 第二步，入队。由于上文中提到线程A已经占用了锁，所以B和C执行tryAcquire失败，并且入等待队列。如果线程A拿着锁死死不放，那么B和C就会被挂起。

先看下入队的过程addWaiter(Node.EXCLUSIVE)：

```
/** * 将新节点和当前线程关联并且入队列 * @param mode 独占/共享 * @return 新节点 */private Node addWaiter(Node mode) { //初始化节点,设置关联线程和模式(独占 or 共享) Node node = new Node(Thread.currentThread(), mode); // 获取尾节点引用 Node pred = tail; // 尾节点不为空,说明队列已经初始化过 if (pred != null) { node.prev = pred; // 设置新节点为尾节点 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 尾节点为空,说明队列还未初始化,需要初始化head节点并入队新节点 enq(node); return node;}
```

B、C线程同时尝试入队列，由于队列尚未初始化，tail==null，故至少会有一个线程会走到enq(node)。我们假设同时走到了enq(node)里。

```
/** * 初始化队列并且入队新节点 */private Node enq(final Node node) { //开始自旋 for (;;) { Node t = tail; if (t == null) { // Must initialize // 如果tail为空,则新建一个head节点,并且tail指向head if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; // tail不为空,将新节点入队 if (compareAndSetTail(t, node)) { t.next = node; return t; } } }}
```

这里体现了经典的**自旋+CAS组合来实现非阻塞的原子操作**。由于compareAndSetHead的实现使用了unsafe类提供的CAS操作，所以只有一个线程会创建head节点成功。假设线程B成功，之后B、C开始第二轮循环，此时tail已经不为空，两个线程都走到else里面。假设B线程compareAndSetTail成功，那么B就可以返回了，C由于入队失败还需要第三轮循环。最终所有线程都可以成功入队。

当B、C入等待队列后，此时AQS队列如下：

![Java锁机制之ReentrantLock实现原理浅析](/img/cef20faa108447b3857a902003211911.jpg)



image-20190319141026958

1. 第三步，挂起。B和C相继执行acquireQueued(final Node node, int arg)。这个方法让已经入队的线程尝试获取锁，若失败则会被挂起。

```
/** * 已经入队的线程尝试获取锁 */final boolean acquireQueued(final Node node, int arg) { boolean failed = true; //标记是否成功获取锁 try { boolean interrupted = false; //标记线程是否被中断过 for (;;) { final Node p = node.predecessor(); //获取前驱节点 //如果前驱是head,即该结点已成老二，那么便有资格去尝试获取锁 if (p == head && tryAcquire(arg)) { setHead(node); // 获取成功,将当前节点设置为head节点 p.next = null; // 原head节点出队,在某个时间点被GC回收 failed = false; //获取成功 return interrupted; //返回是否被中断过 } // 判断获取失败后是否可以挂起,若可以则挂起 if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) // 线程若被中断,设置interrupted为true interrupted = true; } } finally { if (failed) cancelAcquire(node); }}
```

代码里的注释已经很清晰的说明了acquireQueued的执行流程。假设B和C在竞争锁的过程中A一直持有锁，那么它们的tryAcquire操作都会失败，因此会走到第2个if语句中。我们再看下shouldParkAfterFailedAcquire和parkAndCheckInterrupt都做了哪些事吧。

```
/** * 判断当前线程获取锁失败之后是否需要挂起. */private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { //前驱节点的状态 int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 前驱节点状态为signal,返回true return true; // 前驱节点状态为CANCELLED if (ws > 0) { // 从队尾向前寻找第一个状态不为CANCELLED的节点 do { node.prev = pred = pred.prev; } while (pred.waitStatus > 0); pred.next = node; } else { // 将前驱节点的状态设置为SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false;}/** * 挂起当前线程,返回线程中断状态并重置 */private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted();}
```

线程入队后能够挂起的前提是，它的前驱节点的状态为SIGNAL，它的含义是“Hi，前面的兄弟，如果你获取锁并且出队后，记得把我唤醒！”。所以shouldParkAfterFailedAcquire会先判断当前节点的前驱是否状态符合要求，若符合则返回true，然后调用parkAndCheckInterrupt，将自己挂起。如果不符合，再看前驱节点是否>0(CANCELLED)，若是那么向前遍历直到找到第一个符合要求的前驱，若不是则将前驱节点的状态设置为SIGNAL。

 整个流程中，如果前驱结点的状态不是SIGNAL，那么自己就不能安心挂起，需要去找个安心的挂起点，同时可以再尝试下看有没有机会去尝试竞争锁。

 最终队列可能会如下图所示

![Java锁机制之ReentrantLock实现原理浅析](/img/fe8ccb093771467eb214a2ba72f19175.jpg)



image-20190319141139112

**unlock()**

```
public void unlock() { sync.release(1);}public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false;}
```

如果理解了加锁的过程，那么解锁看起来就容易多了。流程大致为先尝试释放锁，若释放成功，那么查看头结点的状态是否为SIGNAL，如果是则唤醒头结点的下个节点关联的线程，如果释放失败那么返回false表示解锁失败。这里我们也发现了，每次都只唤起头结点的下一个节点关联的线程。

最后我们再看下tryRelease的执行过程

```
/** * 释放当前线程占用的锁 * @param releases * @return 是否释放成功 */protected final boolean tryRelease(int releases) { // 计算释放后state值 int c = getState() - releases; // 如果不是当前线程占用锁,那么抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { // 锁被重入次数为0,表示释放成功 free = true; // 清空独占线程 setExclusiveOwnerThread(null); } // 更新state值 setState(c); return free;}
```

这里入参为1。tryRelease的过程为：当前释放锁的线程若不持有锁，则抛出异常。若持有锁，计算释放后的state值是否为0，若为0表示锁已经被成功释放，并且则清空独占线程，最后更新state值，返回free。

用一张流程图总结一下非公平锁的获取锁的过程。

![Java锁机制之ReentrantLock实现原理浅析](/img/edf4884f94804df7a3b0de1be6d9556e.jpg)



image-20190319141224345

**FairSync**

公平锁和非公平锁不同之处在于，公平锁在获取锁的时候，不会先去检查state状态，而是直接执行aqcuire(1

**超时机制**

在ReetrantLock的tryLock(long timeout, TimeUnit unit) 提供了超时获取锁的功能。它的语义是在指定的时间内如果获取到锁就返回true，获取不到则返回false。这种机制避免了线程无限期的等待锁释放。那么超时的功能是怎么实现的呢？我们还是用非公平锁为例来一探究竟。

```
public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout));}
```

还是调用了内部类里面的方法。我们继续向前探究

```
public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);}
```

这里的语义是：如果线程被中断了，那么直接抛出InterruptedException。如果未中断，先尝试获取锁，获取成功就直接返回，获取失败则进入doAcquireNanos()。tryAcquire()我们已经看过，这里重点看一下doAcquireNanos()做了什么。

```
/** * 在有限的时间内去竞争锁 * @return 是否获取成功 */private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { // 起始时间 long lastTime = System.nanoTime(); // 线程入队 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { // 又是自旋! for (;;) { // 获取前驱节点 final Node p = node.predecessor(); // 如果前驱是头节点并且占用锁成功,则将当前节点变成头结点 if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return true; } // 如果已经超时,返回false if (nanosTimeout <= 0) return false; // 超时时间未到,且需要挂起 if (shouldParkAfterFailedAcquire(p, node) && nanosTimeout > spinForTimeoutThreshold) // 阻塞当前线程直到超时时间到期 LockSupport.parkNanos(this, nanosTimeout); long now = System.nanoTime(); // 更新nanosTimeout nanosTimeout -= now - lastTime; lastTime = now; if (Thread.interrupted()) //相应中断 throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); }}
```

doAcquireNanos()的流程简述为：线程先入等待队列，然后开始自旋，尝试获取锁，获取成功就返回，失败则在队列里找一个安全点把自己挂起直到超时时间过期。这里为什么还需要循环呢？因为当前线程节点的前驱状态可能不是SIGNAL，那么在当前这一轮循环中线程不会被挂起，然后更新超时时间，开始新一轮的尝试。

 

# 漫谈IO以及IO性能分析

互联网蚂蚁哥 2019-05-23 17:42:05

## **说说IO（一）- IO的分层**

IO性能对于一个系统的影响是至关重要的。一个系统经过多项优化以后，瓶颈往往落在数据库；而数据库经过多种优化以后，瓶颈最终会落到IO。而IO性能的发展，明显落后于CPU的发展。Memchached也好，NoSql也好，这些流行技术的背后都在直接或者间接地回避IO瓶颈，从而提高系统性能。

**IO系统的分层：**

![漫谈IO以及IO性能分析](/img/5b878380055d4cbb8ac4ebe25bc70502.jpg)



\1. 三层结构

上图层次比较多，但总的就是三部分。**磁盘**（存储）、**VM**（卷管理）和**文件系统**。专有名词不好理解，打个比方说：磁盘就相当于一块待用的空地；

LVM相当于空地上的围墙（把空地划分成多个部分）；文件系统则相当于每块空地上建的楼房（决定了有多少房间、房屋编号如何，能容纳多少人住）；

而房子里面住的人，则相当于系统里面存的数据。

· **文件系统—数据如何存放？**

对应了上图的File System和Buffer Cache。

**File System（文件系统）：解决了空间管理的问题**，即：数据如何存放、如何读取。

**Buffer Cache**：解决数据缓冲的问题。对读，进行cache，即：缓存经常要用到的数据；对写，进行buffer，缓冲一定数据以后，一次性进行写入。

· **VM—磁盘空间不足了怎么办？**

对应上图的Vol Mgmt。

VM其实跟IO没有必然联系。他是处于文件系统和磁盘（存储）中间的一层。**VM屏蔽了底层磁盘对上层文件系统的影响**。当没有VM的时候，文件系统直接使用存储上的地址空间，因此文件系统直接受限于物理硬盘，这时如果发生磁盘空间不足的情况，对应用而言将是一场噩梦，不得不新增硬盘，然后重新进行数据复制。而VM则可以实现动态扩展，而对文件系统没有影响。另外，VM也可以把多个磁盘合并成一个磁盘，对文件系统呈现统一的地址空间，这个特性的杀伤力不言而喻。

· **存储—数据放在哪儿？如何访问？如何提高IO速度？**

对应上图的Device Driver、IO Channel和Disk Device

数据最终会放在这里，因此，效率、数据安全、容灾是这里需要考虑的问题。而提高存储的性能，则可以直接提高物理IO的性能

\2. Logical IO vs Physical IO

逻辑IO是操作系统发起的IO，这个数据可能会放在磁盘上，也可能会放在内存（文件系统的Cache）里。

物理IO是设备驱动发起的IO，这个数据最终会落在磁盘上。

逻辑IO和物理IO不是一一对应的。

## **说IO（二）- IO模型**

这部分的东西在网络编程经常能看到，不过在所有IO处理中都是类似的。

**IO请求的两个阶段**：

**等待资源阶段**：IO请求一般需要请求特殊的资源（如磁盘、RAM、文件），当资源被上一个使用者使用没有被释放时，IO请求就会被阻塞，直到能够使用这个资源。

**使用资源阶段**：真正进行数据接收和发生。

举例说就是**排队**和**服务。**

在**等待数据**阶段，IO分为阻塞IO和非阻塞IO。

**阻塞IO**：资源不可用时，IO请求一直阻塞，直到反馈结果（有数据或超时）。

**非阻塞IO**：资源不可用时，IO请求离开返回，返回数据标识资源不可用

在**使用资源**阶段，IO分为同步IO和异步IO。

**同步IO**：应用阻塞在发送或接收数据的状态，直到数据成功传输或返回失败。

**异步IO**：应用发送或接收数据后立刻返回，数据写入OS缓存，由OS完成数据发送或接收，并返回成功或失败的信息给应用。

![漫谈IO以及IO性能分析](/img/9e2343e3affc4b4f8fe1ab6abfff6c87.jpg)



**按照Unix的5个IO模型划分**

· 阻塞IO

· 非阻塞IO

· IO复用

· 信号驱动的IO

· 异步IO

从性能上看，异步IO的性能无疑是最好的。

**各种IO的特点**

· **阻塞IO**：使用简单，但随之而来的问题就是会形成阻塞，需要独立线程配合，而这些线程在大多数时候都是没有进行运算的。Java的BIO使用这种方式，问题带来的问题很明显，一个Socket需要一个独立的线程，因此，会造成线程膨胀。

· **非阻塞IO**：采用轮询方式，不会形成线程的阻塞。Java的NIO使用这种方式，对比BIO的优势很明显，可以使用一个线程进行所有Socket的监听（select）。大大减少了线程数。

· **同步IO**：同步IO保证一个IO操作结束之后才会返回，因此同步IO效率会低一些，但是对应用来说，编程方式会简单。Java的BIO和NIO都是使用这种方式进行数据处理。

· **异步IO**：由于异步IO请求只是写入了缓存，从缓存到硬盘是否成功不可知，因此异步IO相当于把一个IO拆成了两部分，一是发起请求，二是获取处理结果。因此，对应用来说增加了复杂性。但是异步IO的性能是所有很好的，而且异步的思想贯穿了IT系统方方面面。



## **IO（三）- IO性能的重要指标**





**最重要的三个指标**

**IOPS**

IOPS，即每秒钟处理的IO请求数量。IOPS是随机访问类型业务（OLTP类）很重要的一个参考指标。

· **一块物理硬盘能提供多少IOPS？**

从磁盘上进行数据读取时，比较重要的几个时间是：**寻址时间**（找到数据块的起始位置），**旋转时间**（等待磁盘旋转到数据块的起始位置），**传输时间**（读取数据的时间和返回的时间）。其中寻址时间是固定的（磁头定位到数据的存储的扇区即可），旋转时间受磁盘转速的影响，传输时间受数据量大小的影响和接口类型的影响（不用硬盘接口速度不同），但是在随机访问类业务中，他的时间也很少。因此，在硬盘接口相同的情况下，IOPS主要受限于寻址时间和传输时间。以一个15K的硬盘为例，寻址时间固定为4ms，传输时间为60s/15000*1/2=2ms，忽略传输时间。1000ms/6ms=167个IOPS。

· **OS的一次IO请求对应物理硬盘一个IO吗？**

在没有文件系统、没有VM（卷管理）、没有RAID、没有存储设备的情况下，这个答案还是成立的。但是当这么多中间层加进去以后，这个答案就不是这样了。物理硬盘提供的IO是有限的，也是整个IO系统存在瓶颈的最大根源。所以，如果一块硬盘不能提供，那么多块在一起并行处理，这不就行了吗？确实是这样的。可以看到，**越是高端的存储设备的cache越大，硬盘越多，一方面通过cache异步处理IO，另一方面通过盘数增加，尽可能把一个OS的IO分布到不同硬盘上，从而提高性能**。文件系统则是在cache上会影响，而VM则可能是一个IO分布到多个不同设备上（Striping）。

所以，**一个OS的IO在经过多个中间层以后，发生在物理磁盘上的IO是不确定的。可能是一对一个，也可能一个对应多个**。

· **IOPS能算出来吗？**

对单块磁盘的IOPS的计算没有没问题，但是当系统后面接的是一个存储系统时、考虑不同读写比例，IOPS则很难计算，而需要根据实际情况进行测试。主要的因素有：

·

o **存储系统本身有自己的缓存**。缓存大小直接影响IOPS，理论上说，缓存越大能cache的东西越多，在cache命中率保持的情况下，IOPS会越高。

o **RAID级别**。不同的RAID级别影响了物理IO的效率。

o **读写混合比例**。对读操作，一般只要cache能足够大，可以大大减少物理IO，而都在cache中进行；对写操作，不论cache有多大，最终的写还是会落到磁盘上。因此，100%写的IOPS要越狱小于100%的读的IOPS。同时，100%写的IOPS大致等同于存储设备能提供的物理的IOPS。

o **一次IO请求数据量的多少**。一次读写1KB和一次读写1MB，显而易见，结果是完全不同的。

当时上面N多因素混合在一起以后，IOPS的值就变得扑朔迷离了。所以，一般需要通过实际应用的测试才能获得。

**IO Response Time**

即IO的响应时间。IO响应时间是从操作系统内核发出一个IO请求到接收到IO响应的时间。因此，IO Response time除了包括磁盘获取数据的时间，还包括了操作系统以及在存储系统内部IO等待的时间。一般看，随着IOPS的增加，因为IO出现等待，IO响应时间也会随之增加。对一个OLTP系统，10ms以内的响应时间，是比较合理的。下面是一些IO性能示例：

· **一个8K的IO会比一个64K的IO速度快**，因为数据读取的少些。

· **一个64K的IO会比8个8K的IO速度快**，因为前者只请求了一个IO而后者是8个IO。

· **串行IO会比随机IO快**，因为串行IO相对随机IO说，即便没有Cache，串行IO在磁盘处理上也会少些操作。

需要注意，IOPS与IO Response Time有着密切的联系。一般情况下，IOPS增加，说明IO请求多了，IO Response Time会相应增加。但是会出现IOPS一直增加，但是IO Response Time变得非常慢，超过20ms甚至几十ms，这时候的IOPS虽然还在提高，但是意义已经不大，因为整个IO系统的服务时间已经不可取。

**Throughput**

为吞吐量。这个指标衡量标识了最大的数据传输量。如上说明，**这个值在顺序访问或者大数据量访问的情况下会比较重要**。尤其在大数据量写的时候。

吞吐量不像IOPS影响因素很多，吞吐量一般受限于一些比较固定的因素，如：网络带宽、IO传输接口的带宽、硬盘接口带宽等。一般他的值就等于上面几个地方中某一个的瓶颈。

**一些概念**

**IO Chunk Size**

即单个IO操作请求数据的大小。一次IO操作是指从发出IO请求到返回数据的过程。IO Chunk Size与应用或业务逻辑有着很密切的关系。比如像Oracle一类数据库，由于其block size一般为8K，读取、写入时都此为单位，因此，8K为这个系统主要的IO Chunk Size。**IO Chunk Size**

小，考验的是IO系统的IOPS能力；**IO Chunk Size**大，考验的时候IO系统的IO吞吐量。



**Queue Deep**

熟悉数据库的人都知道，SQL是可以批量提交的，这样可以大大提高操作效率。IO请求也是一样，IO请求可以积累一定数据，然后一次提交到存储系统，这样一些相邻的数据块操作可以进行合并，减少物理IO数。而且Queue Deep如其名，就是设置一起提交的IO请求数量的。一般Queue Deep在IO驱动层面上进行配置。

Queue Deep与IOPS有着密切关系。Queue Deep主要考虑批量提交IO请求，自然只有IOPS是瓶颈的时候才会有意义，如果IO都是大IO，磁盘已经成瓶颈，Queue Deep意义也就不大了。一般来说，IOPS的峰值会随着Queue Deep的增加而增加(不会非常显著)，Queue Deep一般小于256。



**随机访问（随机IO）、顺序访问（顺序IO）**

随机访问的特点是**每次IO请求的数据在磁盘上的位置跨度很大**（如：分布在不同的扇区），因此N个非常小的IO请求（如：1K），必须以N次IO请求才能获取到相应的数据。

顺序访问的特点跟随机访问相反，**它请求的数据在磁盘的位置是连续的**。当系统发起N个非常小的IO请求（如：1K）时，因为一次IO是有代价的，系统会取完整的一块数据（如4K、8K），所以当第一次IO完成时，后续IO请求的数据可能已经有了。这样可以减少IO请求的次数。这也就是所谓的预取。

随机访问和顺序访问同样是有应用决定的。如数据库、小文件的存储的业务，大多是随机IO。而视频类业务、大文件存取，则大多为顺序IO。

**选取合理的观察指标：**

以上各指标中，不用的应用场景需要观察不同的指标，因为应用场景不同，有些指标甚至是没有意义的。

**随机访问和IOPS**: 在随机访问场景下，IOPS往往会到达瓶颈，而这个时候去观察Throughput，则往往远低于理论值。

**顺序访问和Throughput**：在顺序访问的场景下，Throughput往往会达到瓶颈（磁盘限制或者带宽），而这时候去观察IOPS，往往很小。

## **说说IO（四）- 文件系统**



![漫谈IO以及IO性能分析](/img/e258afad455d4a7ba8800bde8348e8c6.jpg)



![漫谈IO以及IO性能分析](/img/74a6bafe19e149b2af0de0abdeab2731.jpg)



文件系统各有不同，其最主要的目标就是解决磁盘空间的管理问题，同时提供高效性、安全性。如果在分布式环境下，则有相应的分布式文件系统。Linux上有ext系列，Windows上有Fat和NTFS。如图为一个linux下文件系统的结构。

其中VFS（Virtual File System）是Linux Kernel文件系统的一个模块，简单看就是一个Adapter，对下屏蔽了下层不同文件系统之间的差异，对上为操作系统提供了统一的接口.

中间部分为各个不同文件系统的实现。

再往下是Buffer Cache和Driver。



![漫谈IO以及IO性能分析](/img/bf3d4377c1de4c4c9119cdef542b3d61.jpg)







**文件系统的结构**

各种文件系统实现方式不同，因此性能、管理性、可靠性等也有所不同。下面为Linux Ext2（Ext3）的一个大致文件系统的结构。

![漫谈IO以及IO性能分析](/img/00aa94eec0784cd8a71f910ee754eb34.jpg)



Boot Block存放了引导程序。

Super Block存放了整个文件系统的一些全局参数，如：卷名、状态、块大小、块总数。他在文件系统被mount时读入内存，在umount时被释放。



![漫谈IO以及IO性能分析](/img/d2eb2a0815bd43a385b68a6f355f2b92.jpg)





上图描述了Ext2文件系统中很重要的三个数据结构和他们之间的关系。

Inode：Inode是文件系统中最重要的一个结构。如图，他里面记录了文件相关的所有信息，也就是我们常说的meta信息。包括：文件类型、权限、所有者、大小、atime等。Inode里面也保存了指向实际文件内容信息的索引。其中这种索引分几类：

· 直接索引：直接指向实际内容信息，公有12个。因此如果，一个文件系统block size为1k，那么直接索引到的内容最大为12k

· 间接索引

· 两级间接索引

· 三级间接索引

如图：



![漫谈IO以及IO性能分析](/img/a62cba51aec74a94a364783931c0568c.jpg)







Directory代表了文件系统中的目录，包括了当前目录中的所有Inode信息。其中每行只有两个信息，一个是文件名，一个是其对应的Inode。需要注意，Directory不是文件系统中的一个特殊结构，他实际上也是一个文件，有自己的Inode，而它的文件内容信息里面，包括了上面看到的那些文件名和Inode的对应关系。如下图：



![漫谈IO以及IO性能分析](/img/46fa0b9c79cb4200a7e86eaafeed588f.jpg)





Data Block即存放文件的时间内容块。Data Block大小必须为磁盘的数据块大小的整数倍，磁盘一般为512字节，因此Data Block一般为1K、2K、4K。

**Buffer Cache**

**Buffer & Cache**

虽然Buffer和Cache放在一起了，但是在实际过程中Buffer和Cache是完全不同了。Buffer一般对于写而言，也叫“缓冲区”，缓冲使得多个小的数据块能够合并成一个大数据块，一次性写入；Cache一般对于读而且，也叫“缓存”，避免频繁的磁盘读取。如图为Linux的free命令，其中也是把Buffer和Cache进行区分，这两部分都算在了free的内存。



![漫谈IO以及IO性能分析](/img/5b156f8f2b5e46bfb92b54118b1e4870.jpg)





**Buffer Cache**

Buffer Cache中的缓存，本质与所有的缓存都是一样，数据结构也是类似，下图为VxSF的一个Buffer Cache结构。



![漫谈IO以及IO性能分析](/img/bfb35d9c92854ee497f5cd8691a23213.jpg)





这个数据结构与memcached和Oracle SGA的buffer何等相似。左侧的hash chain完成数据块的寻址，上方的的链表记录了数据块的状态。

**Buffer vs Direct I/O**

文件系统的Buffer和Cache在某些情况下确实提高了速度，但是反之也会带来一些负面影响。一方面文件系统增加了一个中间层，另外一方面，当Cache使用不当、配置不好或者有些业务无法获取cache带来的好处时，cache则成为了一种负担。

适合Cache的业务：串行的大数据量业务，如：NFS、FTP。

不适合Cache的业务：随机IO的业务。如：Oracle，小文件读取。

**块设备、字符设备、裸设备**

这几个东西看得很晕，找了一些资料也没有找到很准确的说明。

从硬件设备的角度来看，

· 块设备就是以块（比如磁盘扇区）为单位收发数据的设备，它们支持缓冲和随机访问（不必顺序读取块，而是可以在任何时候访问任何块）等特性。块设备包括硬盘、CD-ROM 和 RAM 盘。

· 字符设备则没有可以进行物理寻址的媒体。字符设备包括串行端口和磁带设备，只能逐字符地读取这些设备中的数据。

从操作系统的角度看（对应操作系统的设备文件类型的b和c），

\# ls -l /dev/*lv

brw------- 1 root system 22, 2 May 15 2007 lv

crw------- 2 root system 22, 2 May 15 2007 rlv

· **块设备能支持缓冲和随机读写**。即读取和写入时，可以是任意长度的数据。最小为1byte。对块设备，你可以成功执行下列命令：dd if=/dev/zero of=/dev/vg01/lv **bs=1** count=1。即：在设备中写入一个字节。硬件设备是不支持这样的操作的（最小是512），这个时候，操作系统首先完成一个读取（如1K，操作系统最小的读写单位，为硬件设备支持的数据块的整数倍），再更改这1k上的数据，然后写入设备。

· **字符设备只能支持固定长度数据的读取和写入**，这里的长度就是操作系统能支持的最小读写单位，如1K，所以块设备的缓冲功能，这里就没有了，需要使用者自己来完成。由于读写时不经过任何缓冲区，此时执行dd if=/dev/zero of=/dev/vg01/lv bs=1 count=1，这个命令将会出错，因为这里的bs（block size）太小，系统无法支持。如果执行dd if=/dev/zero of=/dev/vg01/lv **bs=1024** count=1，则可以成功。这里的block size有OS内核参数决定。

如上，相比之下，字符设备在使用更为直接，而块设备更为灵活。文件系统一般建立在块设备上，而为了追求高性能，使用字符设备则是更好的选择，如Oracle的裸设备使用。

**裸设备**

裸设备也叫裸分区，就是没有经过格式化、没有文件系统的一块存储空间。可以写入二进制内容，但是内容的格式、其中信息的组织等问题，需要使用它的人来完成。文件系统就是建立在裸设备之上，并完成裸设备空间的管理。

**CIO**

CIO即并行IO（Concurrent IO）。在文件系统中，当某个文件被多个进程同时访问时，就出现了Inode竞争的问题。一般地，读操作使用的共享锁，即：多个读操作可以并发进行，而写操作使用排他锁。当锁被写进程占用时，其他所有操作均阻塞。因此，当这样的情况出现时，整个应用的性能将会大大降低。如图：



![漫谈IO以及IO性能分析](/img/9fb55571cbb54ea38d509d24d997fdd6.jpg)







CIO就是为了解决这个问题。而且CIO带来的性能提高直逼裸设备。当文件系统支持CIO并开启CIO时，CIO默认会开启文件系统的Direct IO，即：让IO操作不经过Buffer直接进行底层数据操作。由于不经过数据Buffer，在文件系统层面就无需考虑数据一致性的问题，因此，读写操作可以并行执行。

在最终进行数据存储的时候，所有操作都会串行执行，CIO把这个事情交个了底层的driver。



![漫谈IO以及IO性能分析](/img/87e55c0950dc40a8bbbe882ed4786f5d.jpg)









## **说IO（五）- 逻辑卷管理**



LVM（逻辑卷管理），位于操作系统和硬盘之间，LVM屏蔽了底层硬盘带来的复杂性。最简单的，LVM使得N块硬盘在OS看来成为一块硬盘，大大提高了系统可用性。

LVM的引入，使得文件系统和底层磁盘之间的关系变得更为灵活，而且更方便关系。LVM有以下特点：



· 统一进行磁盘管理。按需分配空间，提供动态扩展。

· 条带化（Striped）

· 镜像（mirrored）

· 快照（snapshot）



LVM可以做动态磁盘扩展，想想看，当系统管理员发现应用空间不足时，敲两个命令就完成空间扩展，估计做梦都要笑醒：）

**LVM的磁盘管理方式**

![漫谈IO以及IO性能分析](/img/730b69c81234496cad789a39debc3e1f.jpg)







LVM中有几个很重要的概念：



· **PV（physical volume）**：物理卷。在LVM中，一个PV对应就是操作系统能看见的一块物理磁盘，或者由存储设备分配操作系统的lun。一块磁盘唯一对应一个PV，PV创建以后，说明这块空间可以纳入到LVM的管理。创建PV时，可以指定PV大小，即可以把整个磁盘的部分纳入PV，而不是全部磁盘。这点在表面上看没有什么意义，但是如果主机后面接的是存储设备的话就很有意义了，因为存储设备分配的lun是可以动态扩展的，只有当PV可以动态扩展，这种扩展性才能向上延伸。

· **VG（volume group）**：卷组。一个VG是多个PV的集合，简单说就是一个VG就是一个磁盘资源池。VG对上屏蔽了多个物理磁盘，上层是使用时只需考虑空间大小的问题，而VG解决的空间的如何在多个PV上连续的问题。

· **LV（logical volume）**：逻辑卷。LV是最终可供使用卷，LV在VG中创建，有了VG，LV创建是只需考虑空间大小等问题，对LV而言，他看到的是一直联系的地址空间，不用考虑多块硬盘的问题。

有了上面三个，LVM把单个的磁盘抽象成了一组连续的、可随意分配的地址空间。除上面三个概念外，还有一些其他概念：

· **PE（physical extend）**: 物理扩展块。LVM在创建PV，不会按字节方式去进行空间管理。而是按PE为单位。PE为空间管理的最小单位。即：如果一个1024M的物理盘，LVM的PE为4M，那么LVM管理空间时，会按照256个PE去管理。分配时，也是按照分配了多少PE、剩余多少PE考虑。

· **LE（logical extend）**：逻辑扩展块。类似PV，LE是创建LV考虑，当LV需要动态扩展时，每次最小的扩展单位。

对于上面几个概念，无需刻意去记住，当你需要做这么一个东西时，这些概念是自然而然的。PV把物理硬盘转换成LVM中对于的逻辑（解决如何管理物理硬盘的问题），VG是PV的集合（解决如何组合PV的问题），LV是VG上空间的再划分（解决如何给OS使用空间的问题）；而PE、LE则是空间分配时的单位。

![漫谈IO以及IO性能分析](/img/be27162408204750b121ac3608e8e78f.jpg)





如图，为两块18G的磁盘组成了一个36G的VG。此VG上划分了3个LV。其PE和LE都为4M。其中LV1只用到了sda的空间，而LV2和LV3使用到了两块磁盘。



**串联、条带化、镜像**



![漫谈IO以及IO性能分析](/img/07edcfefe1b9470c8a35c4e7a9689959.jpg)







**串联（Concatenation）:** 按顺序使用磁盘，一个磁盘使用完以后使用后续的磁盘。



**条带化（Striping）:** 交替使用不同磁盘的空间。条带化使得IO操作可以并行，因此是提高IO性能的关键。另外，Striping也是RAID的基础。如：VG有2个PV，LV做了条带数量为2的条带化，条带大小为8K，那么当OS发起一个16K的写操作时，那么刚好这2个PV对应的磁盘可以对整个写入操作进行并行写入。





![漫谈IO以及IO性能分析](/img/dee9dfa4cf534b8792bd1f5e97eaa3f0.jpg)







Striping带来好处有：

· 并发进行数据处理。读写操作可以同时发送在多个磁盘上，大大提高了性能。

Striping带来的问题：



· 数据完整性的风险。Striping导致一份完整的数据被分布到多个磁盘上，任何一个磁盘上的数据都是不完整，也无法进行还原。一个条带的损坏会导致所有数据的失效。因此这个问题只能通过存储设备来弥补。

· 条带大小的设定很大程度决定了Striping带来的好处。如果条带设置过大，一个IO操作最终还是发生在一个磁盘上，无法带来并行的好处；当条带设置国小，本来一次并行IO可以完成的事情会最终导致了多次并行IO。

**镜像（mirror）**

如同名字。LVM提供LV镜像的功能。即当一个LV进行IO操作时，相同的操作发生在另外一个LV上。这样的功能为数据的安全性提供了支持。如图，一份数据被同时写入两个不同的PV。



![漫谈IO以及IO性能分析](/img/7a4922a23fa74b1ea74f954ddab04d38.jpg)







使用mirror时，可以获得一些好处：

· 读取操作可以从两个磁盘上获取，因此读效率会更好些。

· 数据完整复杂了一份，安全性更高。

但是，伴随也存在一些问题:

· 所有的写操作都会同时发送在两个磁盘上，因此实际发送的IO是请求IO的2倍

· 由于写操作在两个磁盘上发生，因此一些完整的写操作需要两边都完成了才算完成，带来了额外负担。

· 在处理串行IO时，有些IO走一个磁盘，另外一些IO走另外的磁盘，一个完整的IO请求会被打乱，LVM需要进行IO数据的合并，才能提供给上层。像一些如预读的功能，由于有了多个数据获取同道，也会存在额外的负担。

**快照（Snapshot）**



![漫谈IO以及IO性能分析](/img/359045c3f4024edabdf93c6f9e959395.jpg)







快照如其名，他保存了某一时间点磁盘的状态，而后续数据的变化不会影响快照，因此，快照是一种备份很好手段。



但是快照由于保存了某一时间点数据的状态，因此在数据变化时，这部分数据需要写到其他地方，随着而来回带来一些问题。关于这块，后续存储也涉及到类似的问题，后面再说。



***说*IO（六）- Driver & IO Channel**



这部分值得一说的是多路径问题。IO部分的高可用性在整个应用系统中可以说是最关键的，应用层可以坏掉一两台机器没有问题，但是如果IO不通了，整个系统都没法使用。如图为一个典型的SAN网络，从主机到磁盘，所有路径上都提供了冗余，以备发生通路中断的情况。

· OS配置了2块光纤卡，分别连不同交换机

· SAN网络配置了2个交换机

· 存储配置了2个Controller，分别连不同交换机

![漫谈IO以及IO性能分析](/img/bb260419413d4ccb9850423f484cdaec.jpg)





如上图结构，由于存在两条路径，对于存储划分的一个空间，在OS端会看到两个（两块磁盘或者两个lun）。可怕的是，OS并不知道这两个东西对应的其实是一块空间，如果路径再多，则OS会看到更多。还是那句经典的话，“计算机中碰到的问题，往往可以通过增加的一个中间层来解决”，于是有了多路径软件。他提供了以下特性：

· 把多个映射到同一块空间的路径合并为一个提供给主机

· 提供fail over的支持。当一条通路出现问题时，及时切换到其他通路

· 提供load balance的支持。即同时使用多条路径进行数据传送，发挥多路径的资源优势，提高系统整体带宽。

Fail over的能力一般OS也可能支持，而load balance则需要与存储配合，所以需要根据存储不同配置安装不同的多通路软件。

多路径除了解决了高可用性，同时，多条路径也可以同时工作，提高系统性能。

## **说说IO（七）- RAID**



Raid很基础，但是在存储系统中占据非常重要的地位，所有涉及存储的书籍都会提到RAID。RAID通过磁盘冗余的方式提高了可用性和可高性，一方面增加了数据读写速度，另一方面增加了数据的安全性。

**RAID 0**

对数据进行条带化。使用两个磁盘交替存放连续数据。因此可以实现并发读写，但带来的问题是如果一个磁盘损坏，另外一个磁盘的数据将失去意义。RAID 0最少需要2块盘。

![漫谈IO以及IO性能分析](/img/a64c4f45c8b3444791097098c2556b70.jpg)



**RAID 1**

对数据进行镜像。数据写入时，相同的数据同时写入两块盘。因此两个盘的数据完全一致，如果一块盘损坏，另外一块盘可以顶替使用，RAID 1带来了很好的可靠性。同时读的时候，数据可以从两个盘上进行读取。但是RAID 1带来的问题就是空间的浪费。两块盘只提供了一块盘的空间。RAID 1最少需要2块盘。

![漫谈IO以及IO性能分析](/img/1c7196634bbb4decb5c4942b12d59328.jpg)



**RAID 5 和 RAID 4**

使用多余的一块校验盘。数据写入时，RAID 5需要对数据进行计算，以便得出校验位。因此，在写性能上RAID 5会有损失。但是RAID 5兼顾了性能和安全性。当有一块磁盘损坏时，RAID 5可以通过其他盘上的数据对其进行恢复。

如图可以看出，右下角为p的就是校验数据。可以看到RAID 5的校验数据依次分布在不同的盘上，这样可以避免出现热点盘（因为所有写操作和更新操作都需要修改校验信息，如果校验都在一个盘做，会导致这个盘成为写瓶颈，从而拖累整体性能，RAID 4的问题）。RAID 5最少需要3块盘。

![漫谈IO以及IO性能分析](/img/222841ebb7014caba1b5ab110481a7ef.jpg)



**RAID 6**

RAID 6与RAID 5类似。但是提供了两块校验盘（下图右下角为p和q的）。安全性更高，写性能更差了。RAID 0最少需要4块盘。

![漫谈IO以及IO性能分析](/img/e30d5cb76f0e4a9d95d1f848c7044d93.jpg)



**RAID 10（Striped mirror）**

RAID 10是RAID 0 和RAID 1的结合，同时兼顾了二者的特点，提供了高性能，但是同时空间使用也是最大。RAID 10最少需要4块盘。

![漫谈IO以及IO性能分析](/img/ef20a70fd0e1442b9f0ea2f9015feefb.jpg)



需要注意，使用RAID 10来称呼其实很容易产生混淆，因为RAID 0+1和RAID 10基本上只是两个数字交换了一下位置，但是对RAID来说就是两个不同的组成。因此，更容易理解的方式是“Striped mirrors”，即：条带化后的镜像——RAID 10；或者“mirrored stripes”，即：镜像后的条带化。比较RAID 10和RAID 0+1，虽然最终都是用到了4块盘，但是在数据组织上有所不同，从而带来问题。RAID 10在可用性上是要高于RAID 0+1的：

· RAID 0+1 任何一块盘损坏，将失去冗余。如图4块盘中，右侧一组损坏一块盘，左侧一组损坏一块盘，整个盘阵将无法使用。而RAID 10左右各损坏一块盘，盘阵仍然可以工作。

· RAID 0+1 损坏后的恢复过程会更慢。因为先经过的mirror，所以左右两组中保存的都是完整的数据，数据恢复时，需要完整恢复所以数据。而RAID 10因为先条带化，因此损坏数据以后，恢复的只是本条带的数据。如图4块盘，数据少了一半。

![漫谈IO以及IO性能分析](/img/1550d9e62d6b41419fc7535a89234728.jpg)



**RAID 50**

RAID 50 同RAID 10，先做条带化以后，在做RAID 5。兼顾性能，同时又保证空间的利用率。RAID 50最少需要6块盘。

![漫谈IO以及IO性能分析](/img/b32dafd741c94bd0a3327a06a263fa23.jpg)



**总结：**

· RAID与LVM中的条带化原理上类似，只是实现层面不同。在存储上实现的RAID一般有专门的芯片来完成，因此速度上远比LVM块。也称硬RAID。

· 如上介绍，RAID的使用是有风险的，如RAID 0，一块盘损坏会导致所有数据丢失。因此，在实际使用中，高性能环境会使用RAID 10，兼顾性能和安全；一般情况下使用RAID 5（RAID 50），兼顾空间利用率和性能；

## **说IO（八）- 三分天下**





**DAS、SAN和NAS**

![漫谈IO以及IO性能分析](/img/24c61a1fb52c4470b186d5315ba454f9.jpg)





为了满足人们不断扩大的需求，存储方案也是在发展的。而DAS、SAN、NAS直接反映这种反映了这种趋势。

· **单台主机**。在这种情况下，存储作为主机的一个或多个磁盘存在，这样局限性也是很明显的。由于受限于主机空间，一个主机只能装一块到几块硬盘，而硬盘空间时受限的，当磁盘满了以后，你不得不为主机更换更大空间的硬盘。

· **独立存储空间**。为了解决空间的问题，于是考虑把磁盘独立出来，于是有了DAS（Direct Attached Storage），即：直连存储。DAS就是一组磁盘的集合体，数据读取和写入等也都是由主机来控制。但是，随之而来，DAS又面临了一个他无法解决的问题——存储空间的共享。接某个主机的JBOD（Just a Bunch Of Disks，磁盘组），只能这个主机使用，其他主机无法用。因此，如果DAS解决空间了，那么他无法解决的就是如果让空间能够在多个机器共享。**因为DAS可以理解为与磁盘交互，DAS处理问题的层面相对更低。使用协议都是跟磁盘交互的协议**

· **独立的存储网络**。为了解决共享的问题，借鉴以太网的思想，于是有了SAN（Storage Area Network），即：存储网络。对于SAN网络，你能看到两个非常特点，一个就是光纤网络，另一个是光纤交换机。**SAN网络由于不会之间跟磁盘交互，他考虑的更多是数据存取的问题，因此使用的协议相对DAS层面更高一些。**

o 光纤网络：对于存储来说，与以太网很大的一个不同就是他对带宽的要求非常高，因此SAN网络下，光纤成为了其连接的基础。而其上的光纤协议相比以太网协议而言，也被设计的更为简洁，性能也更高。

o 光纤交换机：这个类似以太网，如果想要做到真正的“网络”，交换机是基础。

· 网络文件系统。存储空间可以共享，那文件也是可以共享的。NAS（Network attached storage）相对上面两个，看待问题的层面更高，NAS是在文件系统级别看待问题。因此他面的不再是存储空间，而是单个的文件。因此，当NAS和SAN、DAS放在一起时，很容易引起混淆。**NAS从文件的层面考虑共享，因此NAS相关协议都是文件控制协议。**

o NAS解决的是文件共享的问题；SAN（DAS）解决的是存储空间的问题。

o NAS要处理的对象是文件；SAN（DAS）要处理的是磁盘。

o 为NAS服务的主机必须是一个完整的主机（有OS、有文件系统，而存储则不一定有，因为可以他后面又接了一个SAN网络），他考虑的是如何在各个主机直接高效的共享文件；为SAN提供服务的是存储设备（可以是个完整的主机，也可以是部分），它考虑的是数据怎么分布到不同磁盘。

o NAS使用的协议是控制文件的（即：对文件的读写等）；SAN使用的协议是控制存储空间的（即：把多长的一串二进制写到某个地址）



![漫谈IO以及IO性能分析](/img/c13831885f314995ae2a3532ae374b1e.jpg)



如图，对NAS、SAN、DAS的组成协议进行了划分，从这里也能很清晰的看出他们之间的差别。

NAS：涉及SMB协议、NFS协议，都是网络文件系统的协议。

SAN：有FC、iSCSI、AOE，都是网络数据传输协议。

DAS：有PATA、SATA、SAS等，主要是磁盘数据传输协议。

从DAS到SAN，在到NAS，在不同层面对存储方案进行的补充，也可以看到一种从低级到高级的发展趋势。而现在我们常看到一些分布式文件系统（如hadoop等）、数据库的sharding等，从存储的角度来说，则是在OS层面（应用）对数据进行存储。从这也能看到一种技术发展的趋势。

**跑在以太网上的SAN**

SAN网络并不是只能使用光纤和光纤协议，当初之所以使用FC，传输效率是一个很大的问题，但是以太网发展到今天被不断的完善、加强，带宽的问题也被不断的解决。因此，以太网上的SAN或许会成为一个趋势。

· FCIP

如图两个FC的SAN网络，通过FCIP实现了两个SAN网络数据在IP网络上的传输。这个时候SAN网络还是以FC协议为基础，还是使用光纤。

![漫谈IO以及IO性能分析](/img/b055efdde4bd494ea997855213430137.jpg)



· iFCP

通过iFCP方式，SAN网络由FC的SAN网络演变为IP SAN网络，整个SAN网络都基于了IP方式。但是主机和存储直接使用的还是FC协议。只是在接入SAN网络的时候通过iFCP进行了转换

![漫谈IO以及IO性能分析](/img/6d0c753c0e3c44e8bf31d077da6aff5d.jpg)



· iSCSI

iSCSI是比较主流的IP SAN的提供方式，而且其效率也得到了认可。

![漫谈IO以及IO性能分析](/img/e2bc4a077b3e47ce9b06f5b8da0ff7f1.jpg)



对于iSCSI，最重要的一点就是SCSI协议。SCSI（Small Computer Systems Interface）协议是计算机内部的一个通用协议。是一组标准集，它定义了与大量设备（主要是与存储相关的设备）通信所需的接口和协议。如图，SCSI为block device drivers之下。

![漫谈IO以及IO性能分析](/img/f811f4e581c940edb5c131555da7807c.jpg)



从SCIS的分层来看，共分三层：

高层：提供了与OS各种设备之间的接口，实现把OS如：Linux的VFS请求转换为SCSI请求

中间层：实现高层和底层之间的转换，类似一个协议网关。

底层：完成于具体物理设备之间的交互，实现真正的数据处理。

![漫谈IO以及IO性能分析](/img/cab7f9070b544b7e91d474fb61a8dedd.jpg)



 

