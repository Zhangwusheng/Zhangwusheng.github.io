这几天和监控的接触下来,对他们做的东西有了一些了解,下面是我的一些不成熟的想法,和大家探讨一下可能性:

目前的监控的查询需求:维度不多(3个),无论时间查询跨度多大,都需要返回五分钟的汇总数据.

1.目前cdn的监控数据,我感觉都是从监控数据出的,比如群里讨论的:错误率,卡顿率,首屏耗时,dns解析耗时,tcp耗时,首包耗时,不播放离开率;比如运维控制台的监控数据:流量,带宽等数据

业务监控的数据,和日志分析的数据,有些概念上是一致的,监控流量,监控带宽,但是存在下列需要确认的问题:

a)这些日志会有,但是和监控的数据源是否一致,需要深入了解

b)日志分析数据能否替代,也需要了解.

而这些数据是调度的基础.计算好这些数据可以为智能调度提供数据基础.

目前普罗米修斯,已经快要撑不住了,后续的扩展后能不能撑住,我们要不要做提前准备?If True, GoTo Step 2.



2.如果做准备,按照目前的监控需求,五分钟的汇聚.这是基础.

实现方式有如下方式:

2.1)日志分析途径1:

​    做实时计算,将数据控制在五分钟以内完成统计,目前自研非直播速率达标的计算已经完成1分钟的spark的开发,将数据写入到ES,目前哪些域名需要计算达标率,是可以配置的,需求来自于头条,意味着目前域名比较少,数据量可能比较小.根据域名和省份两个维度进行汇总查询.直播的也有1分钟的统计需求(目前未开始,尚未明确具体需求).维度的随意交叉查询,是通过es完成的.大数据量情况下查询性能未知.未做其他时间粒度预先聚合运算.

  优点:时间粒度比较细

  缺点:ES跨长时间段查询性能需测试和检验.

2.2)日志分析途径2:

  五分钟粒度计算,完全按照现在的流程,通过kylin预计算可以完成统计需求.

  缺点:无法支撑更细粒度的计算,加上计算可能需要10分钟以上

  优点:流程稳定,经过验证,预计算能力强.可以查询很长的时间段

2.3) tsdb途径1:

目前的tsdb实现是把所有的数据都写入到一张表,并且按照小时去存储数据,是否能够满足需求?从目前的数据量和查询需求来看,满足需求的压力比较大,主要是扫描的数据比较多.

   2.3.1 是否需要根据实际情况,不同的metrics采用分表策略,比如数据量大的,写入到单独一张表?

   2.3.2 是否可以按照五分钟数据规整,rowkey为五分钟?这样的话,五分钟的聚合,扫描的数据最多也就是时间线的个数,如果针对维度不多的数据,五分钟级别的汇聚,可以很大的减少扫描的数据量,这样五分钟的聚合也可以快速实现.而且哪些指标需要聚合,做成可配置的,不同的指标扫描自己的表,各扫门前雪,可以提高并行度.

​      假如我们将时间戳再规整到20s的粒度(同一个20s内的后到的数据冲掉先到的数据),则可以减少列的存储.

 2.3.3 小时级别的汇总,按照规整到小时做rowkey,小时做偏移量,这样查询和汇聚也很快.写入到小时级别的表

 2.3.4 天级别的从小时级别去计算出数据.

目前的需求是无论查询时间段多长,都是出五分钟的数据.所以需要扫描五分钟级别的表即可.

缺点:定制化开发,不再遵循tsdb那一套规则;查询时计算.

优点:可以满足现有的需求,也可以从五分钟级别表查询清单

2.4)tsdb途径2:

   按照今天的讨论,新建一套kafka接入流程,spark做预计算,将结果通过tsdb接口写入tsdb.目前tsdb不分表,按照小时规整.清单数据也需要写入tsdb.

优点:

通过kafka解耦,接入能力很强

通过spark计算,可定制化程度很高.

缺点:

无论多少维度组合的即席查询,可能需要扫描比较多的无关数据

开发新的流程,不同的指标包含不同的维度,可能spark会有内存大宽表;不同维度的预聚合需要在内存重复计算

备注:spark程序里可以控制不同的metrics是否写同一张表.



抛砖引玉,合适否?请大家提意见.